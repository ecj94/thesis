---
title: 'The reporting of statistical results in sociology: a systematic review'
author: 
- "Elise Schramkowski"
- "Supervisor: Marcel van Assen"
date: "14-02-2022"
bibliography: ref_thesis.bib 
csl: apa.csl
output:
  pdf_document: 
    latex_engine: xelatex
    fig_caption: yes
header-includes:
  - \usepackage{setspace}\doublespacing #double spacing throughout the document
  - \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{S\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}} #resetting the table and figure count for the supplement
  - \usepackage{caption} #customizing captions of tables
fontsize: 12pt
spacing: double
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format = "latex") 
```
\pagenumbering{gobble} 
\pagebreak

# Abstract  
High quality statistical reporting in scientific articles is vital for providing scientific and nonscientific communities with correct information on studied phenomena. In this article, the quality of  statistical reporting in sociology is studied. Firstly, the adherence to statistical reporting guidelines by 143 sociology journals was studied. Furthermore, the presence of statistical reporting errors, publication bias, a ‘bump’ in just significant *p*-values and marginal significance among results of papers were studied. For this purpose, data were automatically retrieved from the 2014-2016 volumes of five sociology journals using the R package statcheck [@statcheck122]. Furthermore, information on these topics was retrieved manually for the 2014-2016 volumes of three sociology journals previously studied in the context of publication bias in sociology by @Gerber2008. It was found that only 13 of these journals (9.1%) adhered to statistical reporting guidelines (i.e., the APA guidelines). No convincing evidence of the presence of publication bias and a ‘bump’ in *p*-values was found. Marginal significance was rather prevalent, especially when manually studying all results of articles related to explicitly stated hypotheses: then, marginal significance was assigned to 81.5% of results with *p*-values in the range (.05-.10]. Across our data, more than 40% of articles contained at least one *p*-values in the (.05-.10] range to which marginal significance was assigned. 
\pagenumbering{arabic} 
\

**Keywords**
statcheck, publication bias, statistical reporting errors, marginal significance, statistical reporting guidelines
\pagebreak

Statistical results in scientific papers provide scientific and nonscientific communities with essential information about studied phenomena. Statistical results should comply with the following standards. Firstly, they should provide sufficient information for reproduction; this will make it easier for readers to critically assess the reported results of a study [@Simera]. Secondly, statistical results should not contain errors, because such inaccuracies in research can lead to incorrect statistical conclusions, placing readers at risk of being misinformed about the nature of studied phenomena. Finally, the reporting of statistical results in papers should be standardized at least within disciplines to enable authors to clearly communicate statistical results and to enable readers to critically evaluate them. In this systematic review, we examined several aspects of statistical results quality in sociology, namely adherence to statistical reporting guidelines, prevalence of statistical reporting errors, evidence of publication bias and *p*-hacking, evidence of a ‘bump’ in just significant *p*-values, and prevalence of *p*-values reported as marginally significant.\
&nbsp;&nbsp;&nbsp;&nbsp;It has been suggested that the presence of statistical reporting guidelines in a discipline may lead to less reporting errors [@Lang]. On the other hand, absence of clear statistical reporting guidelines leads to authors providing insufficient information when reporting statistics, making critical assessment of their results difficult [@Simera]. Thus, having statistical reporting guidelines may well lead to better statistical reporting quality in a discipline. Contrary to psychology, no statistical reporting guidelines have been developed within sociology. Different sociology journals require authors to adhere to different style guidelines, such as the American Psychological Association (APA), American Sociological Association (ASA), Chicago, Harvard, and Oxford style guidelines. Of these style guidelines, only the APA guidelines contain statistical reporting guidelines. We examined which journals request authors to adopt the APA guidelines to evaluate if insufficient and incorrect reporting of results in sociology could be explained by sociology journals not requesting the use of clear statistical reporting guidelines.\
&nbsp;&nbsp;&nbsp;&nbsp;Statistical reporting errors, also called inconsistencies, occur when there is an inconsistency between the following parameters of a reported result: the test statistic, (if used) the degrees of freedom (*df*), and the *p*-value. Inconsistencies are undesirable for two reasons. Firstly, they reflect inaccuracies in reported results. Secondly, they can lead to changes in statistical conclusions based on null hypothesis significance testing (NHST). This can cause audiences to inadvertently decide a true effect exists, or that it does not exist. If an inconsistency leads to changes in statistical conclusions, it is called a gross inconsistency. An example of an inconsistent APA-reported result is ‘*t*(50) = 1.88, *p* = .056’, since *t*(50) = 1.88 implies *p* = .066. An example of a gross inconsistency is ‘*t*(50) = 1.99, *p* = .049’. This suggests a statistically significant result, but *t*(50) = 1.99 implies *p* = .052, which implies that H\textsubscript{0} should not be rejected. To our knowledge, no research on the prevalence of statistical reporting errors in sociology has been conducted at present. However, research in psychology has found that 4%-10% of results are inconsistently reported [@Wicherts; @Nuijten2016]. Gross inconsistencies have been found in 0.8%-2.5% of reported results [e.g., @Veldkamp; @Nuijten2016; @Hartgerink2016]. @Nuijten2016 found that gross inconsistencies occur relatively often in statistically significant reported results; due to gross inconsistencies, the percentage of significant *p*-values among recalculated *p*-values was 2.2 percentage points lower than that found among reported *p*-values (it went from 76.6% to 74.4%). Similarly, @Hartgerink2016 found that of all *p*-values reported as *p* = .05, 67.45% was actually larger than .05. This could point to authors using *p*-hacking and incorrect *p*-value rounding to obtain (false) significance [@John; @Hartgerink2016; @Nuijten2016]. We studied the prevalence of statistical reporting errors in a selection of APA and non-APA journals in two ways: manually, and with the R package statcheck [@statcheck122], which automatically checks the consistency of fully APA-reported results.\
&nbsp;&nbsp;&nbsp;&nbsp;Publication bias occurs when statistically significant results are published relatively more often than non-significant ones. It is one of the suboptimal research/publishing practices that can lead to a relatively high prevalence of just significant *p*-values. In various fields of scientific research, especially in the social and biomedical sciences, publication bias has been found to some extent [e.g., @Dickersin; @Easterbrook; @Fanelli; @Masicampo; @Franco2014; @DeWinter; @Franco2016]. Potential causes of a high prevalence of just significant *p*-values are questionable research practices (QRPs) known as *p*-hacking [@Hartgerink2016; @John; @Lakens2015a; @Masicampo] and the use of researcher degrees of freedom [@Simmons], in which one collects or selects data and/or analyzes results until statistical significance is obtained. Just significant *p*-values can be defined as *p*-values in the range just below the most frequently used threshold for determining significance, *p* = .05. The presence of publication bias and *p*-hacking in sociology was studied by @Gerber2008, who studied statistical results corresponding to hypotheses in all three volumes of the sociology journals American Sociological Review (*ASR*), American Journal of Sociology (*AJS*) and Sociological Quarterly (*SQ*) from 2003-2006. They compared, among others, the number of *z*-values in an interval that closely approximates the *p*-value interval (.04-.05] to the number of *z*-values in an interval that closely approximates the *p*-value interval (.05-.06]. They found that the number of results corresponding to the *p*-value interval (.04-.05] was 3.25 to 4 times higher than that corresponding to the *p*-value interval (.05-.06], presenting strong evidence of publication bias. Similarly, the *p*-value intervals (.04-.05] and (.05-.06] were compared @Masicampo in psychology and by @DeWinter across disciplines. We studied these *p*-value ranges too, and, following @Gerber2008, the *p*-value ranges (.03-.05] and (.05-.07], since larger intervals provide higher power.\
&nbsp;&nbsp;&nbsp;&nbsp;A non-monotonic increase or a ‘bump’ in *p*-values occurs when there are more *p*-values in a just significant *p*-value interval than in the adjacent lower *p*-value interval. It is evidence of *p*-hacking, as publication bias cannot result in a ‘bump’ in *p*-values. Most discipline-specific research on a ‘bump’ in *p*-values has been conducted in psychology, where some studies focusing on *p*-values in the interval (.04-.05] claimed to have found evidence of a ‘bump’ [@Masicampo; @Leggett]. However, according to @Lakens2015b, these studies had not modeled their *p*-value distributions correctly, as they did not take possible publication bias into account. Relatedly, @Hartgerink2016 showed that *p*-hacking does not result in a ‘bump’ if true effect sizes are medium (Cohen’s *d* = 0.5) or larger. Although this implies that the absence of a ‘bump’ is no evidence of absence of *p*-hacking, the presence of a ‘bump’ can only be explained by *p*-hacking. Following @Hartgerink2016, we studied the presence of a ‘bump’ using the *p*-value intervals (.04-.05] versus (.03-.04] and (.03-.05] versus (.01-.03]. Larger intervals were again used because they may provide higher testing power, although power may also decrease because *p*-values slightly larger than .01 will be much more prevalent than *p*-values near .05 in case of true nonzero effects [@Hartgerink2016].\
&nbsp;&nbsp;&nbsp;&nbsp;We also examined the prevalence of results reported as marginally significant in sociology. The reporting of marginally significant results occurs when authors argue that statistically non-significant results (*p* > .05) provide evidence of nonzero true effects, although one can argue they have low evidential value [@Benjamin; @OhlssonCollentine; @Pritschet]. Thus, arguing non-significant results represent true effects may result in (unwarranted) false positives. Since this can lead to audiences assuming a true effect exists while evidence for it is slight, marginally significant *p*-values can be considered undesirable. *P*-values reported as marginally significant can mainly be found in the *p*-value range (.05-.10]; @Pritschet found that of *p*-values reported as marginally significant in psychology, 92.6% were in this interval. @OhlssonCollentine found that almost 40% of *p*-values in the (.05-.10] range retrieved from the text of 44,200 articles of 70 psychology journals were reported as marginally significant. They also found that almost 20% of articles reporting *p*-values contained at least one *p*-value in range (.05-.10] that was reported as marginally significant. As for studies on assignment of marginal significance in sociology, @Leahey found that in 10% of articles from two unnamed top sociology journals from 1995-2000, a significance level of low evidential value of *p* < .10 was used.\
&nbsp;&nbsp;&nbsp;&nbsp;We examined the prevalence of statistical reporting errors, publication bias/*p*-hacking, a ‘bump’ in *p*-values, and *p*-values reported as marginally significant among results of explicitly stated hypotheses (hypotheses referred to in the paper’s text as hypotheses to be tested) and results not related to explicitly stated hypotheses. For statistical reporting errors, publication bias/*p*-hacking, and assignment of marginally significancz, we formally tested whether there were differences in the prevalence of these phenomena between results of explicitly stated hypotheses and other results. One would hope that at least reported results of explicitly stated hypotheses would be without inaccuracies through careful checking by authors before submission and by reviewers and editors before accepting a paper. On the other hand, as publication bias/*p*-hacking is assumed to primarily operate on results related to hypotheses [@Gerber2006], one would expect the prevalence of (gross) inconsistencies, publication bias/*p*-hacking, and marginal significance to be higher among results corresponding to explicitly stated hypotheses. More specifically, we hypothesized the following:
\

*H1: The prevalence of statistical reporting inconsistencies is higher among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses.*
\

*H2: The prevalence of gross statistical reporting inconsistencies is higher among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses.*
\

*H3a: The discrepancy between the amounts of p-values in the intervals (.04-.05] and (.05-.06] is larger among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses.*
\

*H3b: The discrepancy between the amounts of p-values in the intervals (.03-.05] and (.05-.07] is larger among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses.*
\

*H4: The prevalence of p-values in the interval (.05-.10] reported as marginally significant is higher among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses.*
\

We did not construct a hypothesis regarding a possible ‘bump’ in *p*-values. Due to sample sizes generally being larger in sociology than in psychology, statistical power has been suggested to be higher in sociology [@Sedlmeier; @Cohen]. Assuming the same distribution of examined true effects in both fields, higher statistical power implies lower *p*-values on average in sociology. Therefore, and because evidence of a ‘bump’ in psychology is weak at best, we expected neither a ‘bump’ in *p*-values in sociology in general, nor a difference in the presence or size of a ‘bump’ between results related to hypotheses and results not related to hypotheses.
\
\

# Method
## Data sources
For our study on statistical reporting guidelines, we consulted @WOS to create a data set of sociology journals called 'SRG' ('Statistical Reporting Guidelines'). For each journal in 'SRG', we verified whether it adhered to the APA statistical reporting guidelines or not.\
&nbsp;&nbsp;&nbsp;&nbsp;For our study on statistical reporting errors, publication bias/*p*-hacking, the ‘bump’ in *p*-values, and marginal significance, we collected data from articles of several journals. Since statcheck only retrieves and recalculates fully APA-reported results, we collected articles from two sociology journals from @WOS that require APA statistical reporting: Cornell Hospitality Quarterly (*CHQ*) and Journal of Marriage and Family (*JMF*). Of sociology journals requiring APA statistical reporting, these journals were the ones with the highest impact factors from which statcheck could extract results: *CHQ* ranked first with 2.657, *JMF* third with 2.238^[Initially, we had collected articles from *CHQ* and Work and Occupations (*WOX*), which had the second highest impact factor (2.355). However, extracting results from *WOX* articles with statcheck was not possible due to compatibility issues. For an unknown reason, no results could be extracted from neither the HTML nor PDF versions of *WOX* articles.]. We studied all 310 articles from the 2014-2016 volumes of *CHQ* and *JMF* (100 and 210 articles, respectively). To compare differences in statistical reporting errors between APA journals and non-APA journals, we also retrieved results of the 322 articles from the 2014-2016 volumes of three non-APA journals from @WOS: *ASR*, *AJS*, and *SQ*. @Gerber2008 used these in their study on publication bias, which we wanted to replicate. Fully APA-reported results retrieved using statcheck were put into a data set called ‘APA’, and *p*-values retrieved by statcheck were put into a data set called ‘AllP’. Finally, we created a data set called ‘Hyp’, which contained reported *p*-values and statistical results related to explicitly stated hypotheses which were manually retrieved from *ASR*, *AJS*, and *SQ*. This means some fully APA-reported results are also included in ‘Hyp’, as this data set contains all statistical results related to explicitly stated hypotheses. 
\

## Data collection
For each sociology journal in @WOS, we verified if it explicitly required authors to adhere to the APA, ASA, Chicago and/or Harvard style guide and/or another external style guide. We also examined if journals explicitly required authors to follow their own journal’s style guide, and if they allowed authors to follow several different style guides. This information was put into a data set called 'SRG'. There was explicitly required adherence to the own journal’s guidelines if one of the following expressions was found on the journal’s website: 1) ‘House style (guide) *X*’ or ‘Journal style (guide) *X*’, where *X* represents the journal’s name, or 2) ‘*X* (format) requirements’ or ‘*X* (format) requirements’, where again *X* represents the journal’s name. If some form of style guidelines was available, but there was no explicitly named style guide, a journal was put into the category ‘Other’.\
&nbsp;&nbsp;&nbsp;&nbsp;Before extracting statistical information with statcheck, we converted all relevant articles to HTML format. Statcheck namely converts HTML or PDF files to plain text before extracting statistics, and conversion from HTML format is accompanied by less errors [@Nuijten2016]. We then applied statcheck’s ‘checkHTMLdir’ function to a folder with HTML files [@statcheck122] to automatically retrieve APA-reported results, reported *p*-values, and recalculated *p*-values. ‘APA’ contains information retrieved by statcheck on all aspects of fully APA-reported results of all five journals: test statistics (*t*, *z*, *F*, $\chi^2$, and *r*), *df*, and reported *p*-values. Results with exactly reported *p*-values and results with *p*-values reported as ‘<’, ‘>’, or ‘non-significant’ were retrieved by statcheck. If *p*-values were reported as non-significant, statcheck assigned them the label ‘NA’. 'APA' also contains recalculated *p*-values as retrieved by statcheck, as well as information on whether reported *p*-values are (grossly) inconsistent with their recalculated counterparts. If a reported result seemed inconsistent (and this cannot be due to correct rounding), statcheck applied a one-sided test to it. If this lead to a consistent reported result, statcheck kept the one-sided test. Otherwise, it kept the two-sided test [@Nuijten2017]. We also manually put the part of the article’s text from which we concluded that a result was (not) related to an explicitly stated hypothesis in a separate column. Our definition of explicitly stated hypotheses followed that of @Gerber2008, i.e., hypotheses were considered explicitly stated if they were bolded, italicized, or indented, or if they were listed using one of the following terminologies: ‘Hypothesis 1’, ‘H1', ‘H~1~', or ‘the first hypothesis’. 'APA' was used to test our hypotheses on statistical reporting errors (H1 and H2). Since statcheck also retrieved other (incomprehensible) information besides fully APA-reported results, some rows of ‘APA’ were excluded. In total, 505 results from 76 articles were used in descriptive analyses and hypothesis testing (see Table 1 and Table 2). \


```{r table 1 overview AllP APA Hyp (general), echo = FALSE, warning = FALSE, message = FALSE}
library('dplyr') #using pipes
library('knitr') 
library('kableExtra') #making a nice table

#Creating content table cells.
data_table1 <- c("all", "all", "\\emph{ASR}/\\emph{AJS}/\\emph{SQ}", #journals
                 "text", "text", "text/table/figure", #part(s) of article from which info was retraced
                 "partly", "partly", "yes", #results related to explicitly stated hypotheses?
                 471, 80, 91, #total number of articles
                 314, 76, 91, #number of relevant articles
                 7280, 524, 4849, #total number of results
                 2959, 505, 4849) #number of relevant results
#Putting content table cells into a matrix.
table1 <- matrix(data_table1, ncol = 3, nrow = 7, byrow = TRUE)

#Giving matrix row and column names.
rownames(table1) <- paste(c("Journals", "Part(s) of article from which info was retrieved", "Results related to explicitly stated hypotheses?", "Total number of articles", "Number of articles used", "Total number of results", "Number of results used"))
colnames(table1) <- paste(c("‘AllP’", "‘APA’", "‘Hyp’"))

#Creating Table 1.
table1 %>%
  #format latex, centered alignment, bold-faced caption
  kable(booktabs = T, align = "c", caption = "Overview of information provided by ‘AllP', ‘APA', and ‘Hyp'.", escape = FALSE) %>% #centered alignment, bold-faced caption
  kable_styling(latex_options = "HOLD_position", full_width = F)
```

```{r table 2 overview AllP APA Hyp (per topic), echo = FALSE, warning = FALSE, message = FALSE}
#Creating content table cells.
data_table2 <- c("-", "505 (76)", "399 (20)", #descriptive information
                 "-", "505 (76)", "-", #H1 & H2
                 "", "", "",    #blank row
                 64, 38, 14,    #descriptives bump in p-values binwidth .01
                 184, 88, 37,   #descriptives bump in p-values binwidth .02
                 "", "", "",    #blank row
                 73, 28, 14,    #descriptives publication bias binwidth .01
                 127, 56, 26,   #descriptives publication bias binwidth .02
                 "", "", "",    #blank row
                 73, "-", "-",  #H3a
                 127, "-", "-", #H3b
                 "199 (107)", "-", "130 (30)", #descriptives marginal significance
                 "199 (107)", "-", "-")        #H4
#Putting content table cells into a matrix.
table2 <- matrix(data_table2, ncol = 3, nrow = 13, byrow = TRUE)

#Giving matrix row and column names.
rownames(table2) <- paste(c("Descriptive information", "Testing hypotheses (gross) inconsistencies (H1 & H2)", #statistical reporting errors
                            "Descriptive information", "(.03.04] - (.04-.05]", "(.01-.03] - (.03-.05]", #bump in p-values
                            "Descriptive information", "(.04-.05] - (.05-.06]", "(.03-.05] - (.05-.07]", "Testing hypotheses publication bias (H3a & H3b)", "(.04-.05] - (.05-.06]", "(.03-.05] - (.05-.07]", #publication bias
                            "Descriptive information", "Testing hypothesis marginal significance (H4)")) #marginal significance
colnames(table2) <- paste(c("‘AllP'", "‘APA'", "‘Hyp'"))


#Creating Table 2.
table2 <- table2 %>%
  #format latex, centered alignment, bold-faced caption
  kable(booktabs = T, align = "c", caption = "Overview of the numbers of results used in the analyses of article-focused topics for ‘AllP', ‘APA', and ‘Hyp'.") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position", "scale_down"), font_size = 15, full_width = F) 

table2 %>%
  #adding indentation for different parts of topics
  add_indent(c(4, 5, 7, 8, 10, 11)) %>%
  #grouping data for the different topics
  pack_rows("Statistical reporting errors", 1, 2) %>%
  pack_rows("Bump in \\emph{p}-values", 3, 5, escape = FALSE) %>%
  pack_rows("Publication bias", 6, 11) %>%
  pack_rows("Marginal significance", 12, 13) %>%
  #adding footnote
  add_footnote("Note. Numbers of articles from which results were used in analyses are shown between parentheses.", notation = "none")
```
\

&nbsp;&nbsp;&nbsp;&nbsp;The third dataset, ‘AllP’, consists of all reported *p*-values of all five journals retrieved by statcheck. We manually added information on whether reported *p*-values were related to an explicitly stated hypothesis as we did for ‘APA’. Of 7,280 results retrieved by statcheck, we removed 4,354 (59.8%) because they did not refer to reported *p*-values. Thus, 'APA' contained 2,926 reported *p*-values from 308 articles. From these data, descriptive information on publication bias, the ‘bump’ in *p*-values, and marginal significance was obtained. Furthermore, these data were used to test H3a, H3b, and H4 (see Table 1 and Table 2 for an overview). To determine if marginal significance was assigned to a reported *p*-value, we looked up *p*-values in the (.05-.10] range in the text of articles. Then, following @OhlssonCollentine, we decided that a *p*-value was assigned marginal significance by authors if the expressions ‘margin\*’ or ‘approach\*’ were mentioned in relation to its significance. The text used to conclude that a *p*-value was (not) assigned marginal significance was stored manually in a separate column of ‘AllP’. Finally, we obtained descriptive statistics on articles with at least one *p*-value in the range (.05-.10] to which marginal significance was assigned.\
&nbsp;&nbsp;&nbsp;&nbsp;A fourth data set, ‘Hyp’, was created to replicate the study of @Gerber2008 on publication bias by manually retrieving results from articles. Manual retrieval allows one to retrieve information from tables, figures, and text, whereas statcheck can only retrieve information from text. We only collected data from articles that met our inclusion criteria. Like @Gerber2008, we only studied articles that explicitly stated one or more hypotheses before their results were presented. Of the 322 articles in *ASR*, *AJS* and *SQ*, 99 (30.7%) met this criterion. Furthermore, articles had to contain at least one 'required statistic', i.e., at least one *p*-value or reproducible result related to an explicitly stated hypothesis. This was the case for 91 articles (28.3%) (see Figure 1 for an overview of the selection process). Following @Gerber2008, ‘Hyp’ contains all relevant results from all models used to test explicitly stated hypotheses. Information from appendices was also included, but information from supplements was not, since only appendices are part of articles as published. There are also some differences between our study and that of @Gerber2008. @Gerber2008 used caliper tests for *z*-distributions consisting of *z*-values and *t*-values (converted to *z*-values) within 5%, 10%, 15% or 20% of *z* = 1.64 (one-sided testing) or *z* = 1.96 (two-sided testing). If *z*-values or *t*-values were unavailable, regression coefficients and standard errors were used to calculate *z*-values. We used exactly reported *p*-values in the ranges (.04-.06] and (.03-.07] instead, since it was often unknown what kind of distribution an analysis was based on. Also, this allowed us to include *p*-values based on *F*-values, *r*-values, and $\chi^2$-values. Finally, @Gerber2008 excluded articles with more than 38 relevant coefficients because their inclusion could have a disproportionate impact on analyses. We did not do so since we wanted to include all *p*-values relevant for studying publication bias. If one or more articles would influence the results disproportionately, we would do extra analyses without these articles. We organized all aspects of a result of an explicitly stated hypothesis - *p*-value, regression coefficient (or odds ratio, proportional hazard, etc.), *z*-value, *t*-value, *F*-value, *r*-value, $\chi^2$-value, standard error, phrasing of the hypothesis a result belonged to as retrieved from the article, and, if applicable, text from the article in which a result was mentioned - as we did for ‘APA’. In total, ‘Hyp’ contained 4,929 results. We used 'Hyp' to study all our results-level phenomena (see Table 2). Where possible, we checked whether statistical results were (grossly) inconsistent by recalculating their *p*-values. For information on how this was done, see Table 3 and Table 4. We also manually added information on assignment of marginal significance to in-text *p*-values in the range (.05-.10] to ‘Hyp’ as we did for ‘AllP’. For *p*-values in tables, we considered significance levels of *p* < .10 in captions (indicated by, e.g., an asterisk) to be assignment of marginal significance. Finally, we studied the percentage of articles in ‘Hyp’ containing marginally significant results in the range (.05-.10]. Note that ‘AllP’, ‘APA’, and ‘Hyp’ overlap. For instance, an in-text APA-reported result related to an explicitly stated hypothesis is included in all three data sets.
\

<center>

![Flowchart describing the process of selecting articles from which results were retrieved for ‘Hyp’.](Flowchart_thesis.png)

</center>

&nbsp;
$$\\[10in]$$
&nbsp;

```{r table 3 definition (gross) inconsistencies per type of reported p-value, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating contents table cells.
data_table3 <- c("ns", "Cal\\emph{P} $\\leq$ .05", "Cal\\emph{P} $\\leq$ .05", #nonsignificant results
                 "$<$", "Cal\\emph{P} $\\geq$ Rep\\emph{P}", "Cal\\emph{P} $>$ .05 \\& Rep\\emph{P} $\\leq$ .05", #results with '<'
                 "$\\geq$", "Cal\\emph{P} $<$ Rep\\emph{P}", "Cal\\emph{P} $<$ .05 \\& Rep\\emph{P} $\\geq$ .05", #results with '≥'
                 "=", " Cal\\emph{P} $\\neq$ Rep\\emph{P} not due to", "Cal\\emph{P} $\\neq$ Rep\\emph{P} not due to rounding,", "", "rounding\\textsuperscript{*}", "Cal\\emph{P} $\\leq$ .05 \\& Rep\\emph{P} $>$ .05 or vice versa") #results with '-'
#Putting content table cells into a matrix.
table3 <- matrix(data_table3, ncol = 3, nrow = 5, byrow = TRUE)
#Giving matrix column names.
colnames(table3) <- paste(c("Type of Rep\\emph{P}", "Inconsistent if...", "Grossly inconsistent if..."))

#Creating Table 3.
table3 %>%
  #format latex, centered alignment, bold-faced caption
  kable(booktabs = T,  align = "c", caption = "Conditions under which different types of reported \\emph{p}-values from ‘Hyp’ were considered (grossly) inconsistent.", escape = FALSE) %>% 
  #table will appear in correspondence with its place in this document, table is scaled-down to fit page size
  kable_styling(latex_options = c("HOLD_position", "scale_down"), full_width = F) %>% 
  #Adding footnotes
  add_footnote("Note. Cal\\emph{P} = recalculated \\emph{p}-value, Rep\\emph{P} = reported \\emph{p}-value, ns = non-significant.", notation = "none", escape = FALSE)  %>% 
  add_footnote("See Table 4 for methods used to determine whether a difference between recalculated and reported \\emph{p}-values", notation = "symbol", escape = FALSE) %>%
  add_footnote("could be due to correct rounding or not.", notation = "none")  %>%
  #Specifying column length and font size
  column_spec(1,width = "14cm") %>%
  column_spec(2, width = "18cm")%>%
  column_spec(3, width = "26cm")%>%
  #specifying font size
  kable_styling(font_size = 40)
```



```{r table 4 difference exactly reported and recalculated p-values (not) due to rounding, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
data_table4 <- c(
##b & SE
"Only used for recalculation if a result was explicitly based on the \\emph{z}-distribution or \\emph{t}-distribution.", "We use \\emph{b} $=$ 3.11, SE $=$ 2.11, \\emph{p} $=$ 0.07, from a \\emph{z}-distribution as an example:", #intro b&SE
                 
"- Correct Cal\\emph{P} will stem from \\emph{b}\\textsubscript{lb} $\\leq$ \\emph{b} $<$ \\emph{b}\\textsubscript{ub} (e.g., 3.105 $\\leq$ \\emph{b} $<$ 3.115) and SE\\textsubscript{lb} $\\leq$ SE $<$ SE\\textsubscript{ub}" , "(e.g., 2.105 $\\leq$ SE $<$ 2.115).", #correct intervals
                 
"- Calculate \\emph{t}/\\emph{z}\\textsubscript{ub} $=$ $\\frac{\\emph{b} \\textsubscript{ub}}{SE\\textsubscript{lb}}$ and \\emph{t}/\\emph{z}\\textsubscript{lb} $=$ $\\frac{\\emph{b} \\textsubscript{lb}}{SE\\textsubscript{ub}}$, the largest and smallest \\emph{t}/\\emph{z} consistent with \\emph{b} and SE", #calculation t/z
                 "(e.g., \\emph{z}\\textsubscript{ub} $=$ $\\frac{3.115}{2.105}$ $=$ 1.47981 and \\emph{z}\\textsubscript{lb} $=$ $\\frac{3.105}{2.115}$ $=$ 1.468085).", #example calculation t/z

"- Use \\emph{t}/\\emph{z}\\textsubscript{lb} and \\emph{t}/\\emph{z}\\textsubscript{ub} (and, in case of \\emph{t}, reported \\emph{df}) to calculate Cal\\emph{P}\\textsubscript{lb} and Cal\\emph{P}\\textsubscript{ub} (boundaries", "of correctly rounded Rep\\emph{P}). For this, the R stats package pt() function - in case of \\emph{t} - or the", "pnorm() function - in case of \\emph{z} - is used. Then, Cal\\emph{P}\\textsubscript{lb} and Cal\\emph{P}\\textsubscript{ub} are rounded to the same", #calculation Callb and Calub
                  "number of decimals as Rep\\emph{P} with R base round() function. In our example, Cal\\emph{P}\\textsubscript{lb} $\\approx$ 0.07", "and Cal\\emph{P}\\textsubscript{ub} $\\approx$ 0.07.", #example calculation Callb and Calub

"- If Cal\\emph{P}\\textsubscript{lb} $\\leq$ Rep\\emph{P} $\\leq$ Cal\\emph{P}\\textsubscript{ub}, Rep\\emph{P} is considered correct. This is the case in our example.", #conclusion

"", #white space row

##test statistics
"Functions of the R stats package used for recalculation of \\emph{p}-values: pt() for \\emph{t} and \\emph{r}, pnorm() for \\emph{z},", "pf() for \\emph{F}, and chisq() for $\\chi^2$. All functions, except pnorm(), require information on \\emph{df}. We use ", #intro
                 "the example of \\emph{t}(61) $=$ 3.11, \\emph{p} $=$ 0.0001:", #intro example

"- Correct Cal\\emph{P} will stem from \\emph{t}\\textsubscript{lb} $\\leq$ \\emph{t} $<$ \\emph{t}\\textsubscript{ub} (e.g., 3.105 $\\leq$ \\emph{t} $<$ 3.115).", #correct intervals

"-	Calculate the \\emph{p}-values consistent with the highest and lowest \\emph{t}-values possible under correct", "rounding with pt() function, and round Cal\\emph{P}\\textsubscript{lb} and Cal\\emph{P}\\textsubscript{ub} to the same number of decimals as ", #calculation Callb and Calub
                 "Rep\\emph{P} with R base round() function. In our example, Cal\\emph{P}\\textsubscript{lb} $\\approx$ 0.001 and Cal\\emph{P}\\textsubscript{ub} $\\approx$ 0.001.", #example calculation Callb and Calub

"- If Cal\\emph{P}\\textsubscript{lb} $\\leq$ Rep\\emph{P} $\\leq$ Cal\\emph{P}\\textsubscript{ub}, Rep\\emph{P} is considered correct. This is not the case in our example,", #conclusion
                 "since Rep\\emph{P} is smaller than Cal\\emph{P}\\textsubscript{lb} and Cal\\emph{P}\\textsubscript{ub}.", #conclusion example

"" #white space row
                 )

#Putting content table cells into a matrix.
table4 <- matrix(data_table4, ncol = 1, nrow = 22, byrow = TRUE)

#Creating Table 4.
table4 %>%
  #format latex, alignment to the left, bold-faced caption
  kable(booktabs = T,  align = "l", caption = "Ways of determining whether discrepancies between exactly reported \\emph{p}-values and their recalculated counterparts from  ‘Hyp' could be due to correct rounding or are indicative of an inconsistency.", escape = FALSE) %>% 
  #grouping data for the different methods of recalculation
  pack_rows("b & SE", 1,13) %>%
  pack_rows("test statistics", 14, 22) %>%
  #Adding indentation 
  add_indent(c(3:12, 17:22)) %>% 
  #table will appear in correspondence with its place in this document, table is scaled-down to fit page size, font size is 20
  kable_styling(latex_options = c("HOLD_position", "scale_down"), full_width = F) %>%
  kable_styling(font_size = 20) %>% 
  #adding footnotes
  add_footnote("Note. Rep\\emph{P} = reported \\emph{p}-value, Cal\\emph{P} = recalculated \\emph{p}-value, Cal\\emph{P}\\textsubscript{lb} = lower bound recalculated \\emph{p}-value," , notation = "none", escape = FALSE)  %>% 
  add_footnote("Cal\\emph{P}\\textsubscript{ub} = upper bound recalculated \\emph{p}-value, \\emph{b}\\textsubscript{lb} = lower bound \\emph{b}, \\emph{b}\\textsubscript{ub} = upper bound \\emph{b}, SE\\textsubscript{lb} = lower", notation = "none", escape = FALSE) %>% 
  add_footnote("bound SE, SE\\textsubscript{ub} = upper bound SE.", notation = "none", escape = FALSE) 
```


## Statistical analyses
In our descriptive analyses (which consist of frequencies and percentages), we reported how many journals from 'SRG' require authors to adhere to the APA statistical reporting guidelines. For (gross) inconsistencies, descriptive results were based on ‘APA’ and ‘Hyp’. We followed @Nuijten2016 by studying the direction of gross inconsistencies: do errors make non-significant results significant, or vice versa? For publication bias, the ‘bump’ in *p*-values, and marginal significance, descriptive results were based on ‘AllP’ and ‘Hyp’. For marginal significance, these data sets also provided descriptive statistics at the article level. For all research topics but statistical reporting guidelines, descriptive results were split by explicitly stated hypothesis (yes/no), journal (*ASR*, *AJS*, *SQ*, and, for parts of the study using statcheck, *CHQ*, and *JMF*), and year (2014, 2015, 2016).   
&nbsp;&nbsp;&nbsp;&nbsp;Nuijten et al. (2017) have argued that the prevalence of (gross) inconsistencies can be studied in three ways. Firstly, one can calculate the percentage of inconsistencies and gross inconsistencies for each article and take the average of these percentages over all articles. Secondly, one can calculate the overall percentage of (gross) inconsistencies by dividing the amount of (gross) inconsistencies by the total number of reported results obtained. Thirdly, one can use multilevel logistic regression models to estimate the probability that a reported result is inconsistent, while controlling for the nesting of results within articles. Although in theory, the third method is most appropriate, simulation analyses revealed that it performs poorly; because both the number of results per article and the probability of a gross inconsistency are too low, it is accompanied by a too low Type I error, a lack of statistical power, and clearly inaccurate effect size estimates [@Nuijten2017]. Therefore, following @Wicherts and @Nuijten2016, we tested our hypotheses on statistical reporting errors (H1 and H2) using logistic regressions.\
&nbsp;&nbsp;&nbsp;&nbsp;We conducted logistic regressions to test our hypothesis on publication bias (H3a, H3b) with exactly reported *p*-values from ‘AllP’ as the dependent variable. Since statcheck interprets results with *p* = .05 as being statistically significant [@statcheck122], *p* = .05 was included in the interval of just significant *p*-values for the logistic regressions. H3a was tested using the *p*-value intervals (.04-.05] and (.05-.06] to obtain more precise results, and H3b was tested using *p*-value intervals (.03-.05] and (.05-.07] for a potentially more powerful test.\
&nbsp;&nbsp;&nbsp;&nbsp;To test our hypothesis on *p*-values reported as marginally significant (H4), we conducted logistic regressions with exactly reported *p*-values in the range (.05-.10] from ‘AllP’ as the dependent variable.\
&nbsp;&nbsp;&nbsp;&nbsp;All logistic regression analyses contained a binary predictor indicating whether a result was related to an explicitly stated hypothesis or not. We chose not to include other potentially relevant control variables, such as journal and year of publication, because some analyses had too little data for including multiple predictors. 
\
\

# Results
In this section, we start by presenting our results regarding statistical reporting guidelines. Next, results on statistical reporting errors, publication bias/*p*-hacking, the ‘bump’ in *p*-values, and marginal significance are discussed. For each results-level topic, we first present automatically retrieved descriptive results and (if applicable) results of hypothesis testing. Then, we discuss descriptive statistics of results related to explicitly stated hypotheses from 'Hyp'. Results on specific years and journals that were of little theoretical interest or were based on too little data are not discussed in text, but can be found in the corresponding tables. Full tables of the results of logistic regressions can be found in the supplement.
\

## Statistical reporting guidelines
Of the 143 sociology journals in 'SRG', one journal (Society) did not seem to have any explicit guidelines authors are required or allowed to follow when preparing their manuscripts. Four journals (2.8%) explicitly required authors to follow guidelines established by the journal itself, and 102 (71.3%) required authors to adhere to (reference) guidelines established by external organizations. Only 13 journals (9.1%) requested authors to adhere to the APA manual, and thereby, to the APA statistical reporting guidelines. See Table 5 for an overview of the numbers of sociology journals requesting/allowing adherence to different categories of guidelines.
\

```{r table5 statistical reporting guidelines, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
data_table5 <- c("", "10 (7%)", "10 (7%)",
                 "", "12 (8.4%)", "3 (2.1%)",
                 "", "7 (4.9%)", "6 (4.2%)",
                 "", "2 (1.4%)", "9 (6.3%)",
                 "1 (0.7%)", "1 (0.7%)", "1 (0.7%)",
                 "34 (23.8%)", "4 (2.8%)", "1 (0.7%)",
                 "37 (25.9%)", "3 (2.1%)", "1 (0.7%)",
                 "1 (0.7%)", "1 (0.7%)", "143 (100%)")
#Putting content table cells into a matrix.
table5 <- matrix(data_table5, ncol = 1, nrow = 24, byrow = TRUE)

#Giving the matrix row names and column names.
rownames(table5) <- c("APA", "Full manual", "Only references", 
                            "ASA", "Full manual", "Only references",  
                            "Chicago", "Full manual", "Only references",  
                            "Harvard", "Full manual", "Only references",   
                            "Oxford", "Style Manual for Authors, Editors and Printers", 
                            "Wiley", "Other", "Own", "Multiple options (one must be chosen)",
                            "Multiple required", "Multiple required (one is full APA manual)", "Multiple allowed",
                            "None mentioned", "Unknown*", "Total")
colnames(table5) <- paste(c("Number of journals (% of total)"))

#Creating Table 5.
table5 %>%
  #format latex, centered alignment, bold-faced caption
  kable(booktabs = T, align = "c", longtable = TRUE, caption = "Numbers and percentages of sociology journals in ‘SRG' requesting/allowing adherence to different types of statistical reporting guidelines.") %>% 
  #grouping data
  pack_rows("Required", 1, 20) %>%
  pack_rows("Other, namely...", 21, 23) %>%
  #adding indentation for different groups
  add_indent(c(2:3, 5:6, 8:9, 11:12)) %>%
  #table will appear in correspondence with its place in this document, table is scaled-down to fit page size, font size is 10
  kable_styling(latex_options = c("HOLD_position", "repeat_header"), full_width = F) %>%
  kable_styling(font_size = 10) %>% 
  #Adding footnote
  footnote(symbol = "We were unable to find which guidelines authors publishing in the journal Society are required or allowed to use. The link provided on the website that should give access to this information gave a ‘page not found’ error.", escape = FALSE, threeparttable = T, footnote_as_chunk = T)
```

## Statistical reporting errors
Of the 505 ‘APA’ results, 69 (13.7%) were inconsistent and 8 (1.6%) grossly inconsistent (see Table 6). All grossly inconsistent results had a statistically significant reported *p*-value and a non-significant recalculated *p*-value, making the percentage of significant *p*-values in recalculated *p*-values 1.6 percentage points lower (30.3% rather than 31.9%) than that in reported *p*-values. Out of 168 results related to explicitly stated hypotheses, 22 (13.1%) were inconsistent and 4 (2.4%) grossly inconsistent. Out of 337 results not related to explicitly stated hypotheses, 47 (13.9%) were inconsistent and 4 (1.2%) grossly inconsistent. Our hypotheses that less (gross) inconsistencies will be observed for results on explicitly stated hypotheses are not confirmed. As for H1, the odds of a result of an explicitly stated hypothesis being inconsistent were 1.076 times smaller than the odds that a result not related to an explicitly stated hypothesis was inconsistent, *b* = -.073, *p* = .793, OR = .930, 95% CI [.531, 1.585]. Regarding H2, the odds of a result of an explicitly stated hypothesis being grossly inconsistent were 2.030 two times larger than the odds that a result not related to an explicitly stated hypothesis was grossly inconsistent, but this difference was not significant, *b* = .708, *p* = .321, OR = 2.030, 95% CI [.475, 8.685]. Of the recalculated *p*-values, 416 (82.4%) were retrieved from the two APA journals. These journals, *JMF* and *CHQ*, had very similar percentages of inconsistencies (14.6% and 14.7%, respectively) and gross inconsistencies (1.6% and 1.7%, respectively). As for reproducible results from ‘Hyp’, 17 out of 399 recalculated *p*-values were inconsistent (4.3%), and three (0.8%) were grossly inconsistent. For a comprehensive overview of results, see Table 6.


```{r table6 descriptives (gross) inconsistencies, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
data_table6 <- c("", "", "", "", 
                 29, 168, "22 (13.1%)", "4 (2.4%)", #begin APA
                 68, 337, "47 (13.9%)", "4 (1.2%)", 
                 "", "", "", "", 
                 7, 43, "1 (2.3%)", "1 (2.3%)",
                 3, 41, "2 (4.9%)", "0 (0%)",
                 2, 5, "5 (100%)", "0 (0%)", 
                 36, 185, "27 (14.6%)", "3 (1.6%)",
                 28, 231, "34 (14.7%)", "4 (1.7%)",
                 "", "", "", "", 
                 20, 172, "22 (12.8%)", "1 (0.6%)", 
                 21, 136, "15 (11.0%)", "3 (2.2%)",
                 35, 197, "32 (16.2%)", "4 (2.0%)", 
                 76, 505, "69 (13.7%)", "8 (1.6%)", #end APA
                 "", "", "", "",                    #begin Hyp
                 11, 313, "13 (4%)", "2 (0.6%)",
                 7, 68, "2 (2.9%)", "1 (1.5%)", 
                 2, 5, "2 (40.0%)", "0 (0%)", 
                 "", "", "", "", 
                 12, 307, "12 (3.9%)", "2 (0.7%)", 
                 3, 33, "1 (3.0%)", "0 (0%)", 
                 5, 59, "3 (5.1%)", "1 (1.7%)", 
                 20, 399, "17 (4.3%)", "3 (0.8%)") #end Hyp
#Putting content table cells into a matrix.
table6 <- matrix(data_table6, ncol = 4, nrow = 23, byrow = TRUE)

#Giving matrix row and column names.
rownames(table6) <- c("Relation to hypothesis", "Yes", "No", "Journal", 
                            "ASR", "AJS", "SQ", "JMF", "CHQ", "Year", 
                            "2014", "2015", "2016", "Total", "Journal", 
                            "ASR", "AJS", "SQ", "Year", 
                            "2014", "2015", "2016", "Total")
colnames(table6) <- paste(c("Articles", "Results", "Inconsistencies", "Gross inconsistencies"))

#Creating Table 6.
table6 %>%
  #format latex, centered alignment, bold-faced caption
  kable(booktabs = T, align = "c", caption = "Descriptive statistics on (gross) inconsistencies for ‘APA' and ‘Hyp'.") %>% 
  #adding indentation for the categories of all variables
  add_indent(c(2,3, 5:9, 11:13, 16:18, 20:22)) %>% 
  #grouping data for the two data sets
  pack_rows("‘APA'", 1, 14) %>%
  pack_rows("‘Hyp'", 15, 23) %>%
  #specifying width of columns (gross) inconsistencies
  column_spec(3, width = "5em") %>%
  column_spec(4, width = "5em") %>%
  #table will appear in correspondence with its place in this document, table is scaled-down to fit page size
  kable_styling(latex_options = c("striped", "HOLD_position", "scale_down"), full_width = F) %>%
  #adding footnotes
  add_footnote("Note. The numbers of articles for the results (not) related to explicitly stated hypotheses reflect the", notation = "none") %>%
  add_footnote("numbers of articles that contain at least one result that is (not) related to an explicitly stated hypothesis.", notation = "none") 
```

## Publication bias	
In ‘AllP’, there was no evidence of publication bias/*p*-hacking (see Figure 2A and Table 7). Overall, when using binwidth .01, 31 out of 73 results were just significant (43.8%). When using binwidth .02, 64 out of 127 results were just significant (50.4%). A non-substantial indication of publication bias was found for results not related to explicitly stated hypotheses when using binwidth .02: out of 90 results, 47 (52.2%) were just significant). There was no evidence of publication bias among results related to explicitly stated hypotheses (see Figure 2C). Next, we tested our hypotheses on publication bias (H3a and H3b). As for H3a, there were $\frac{1}{.877} \approx 1.140$ times less just significant *p*-values among reported results of explicitly stated hypotheses than among reported results not related to explicitly stated hypotheses for binwidth .01, *b* = -.132, *p* = .794, OR = .877, 95% CI [.321, 2.345]. H3b for binwidth .02 could not be rejected either, since *b* = -.251, *p* = .521, OR = .778, 95% CI [.358, 1.674]. For results from ‘Hyp’, we found (slightly) more just significant *p*-values than just insignificant ones: 9 out of 14 results (64.3%) for binwidth .01, and 14 out of 27 (53.8%) for binwidth .02, see Figure 2D and Table 7. 

```{r plots AllP, warning=FALSE, message = FALSE, echo = FALSE, results = 'hide', fig.height=8, fig.cap="Histograms of exactly reported \\emph{p}-values in the range [0-.10] from ‘AllP’ and 'Hyp'. Specifically, information on 'AllP' is provided for all exactly reported \\emph{p}-values in the range [0-.10] (2A), as well as for exactly reported \\emph{p}-values in this range (not) related to explicitly stated hypotheses (2B and 2C). Finally, information on exactly reported \\emph{p}-values in the range [0-.10] from 'Hyp' (2D) is provided."}
library('readxl') #reading in data files
library('ggplot2') #plotting
library('ggtext') #adapting title size
library('gridExtra') #arranging tables
library('gridtext') #adapting text above final table
library('grid') #font (size) options in grid.arrange function
library('ggpubr') #creating a ggplot object out of multiple ggplots out together
library("cowplot") #adding a label to the final plot


############################
##        Plots AllP      ##  
############################

setwd("..") #setting working directory to 'thesis'
setwd("./Data") #setting working directory to 'Data'

#Reading in 'AllP'
AllP <- read_excel("./AllP/AllP.xlsx", col_types = c(
#Article_numb, Article, Author(s), Journal
"numeric", "text", "text", "text", 
#Year, Statistic, Reported.Comparison, Reported.P.Value
"numeric", "text", "text", "numeric",
#Raw, Result.Table, Reported.Not.Relevant, Not.Reported
"text", "numeric", "numeric", "numeric",
#Model.fit, Result.Hypothesis, Marg.Sig, Phrase in which mentioned
"numeric", "numeric", "numeric", "text", 
#explicitly stated hypothesis, Comments
"text", "text"))

#Assigning labels to the categories of 'Result.Hypothesis'.
AllP$Result.Hypothesis[which(AllP$Result.Hypothesis == 0)] <- "Result not related to hypothesis"
AllP$Result.Hypothesis[which(AllP$Result.Hypothesis == 1)] <- "Result related to hypothesis"
#Selecting only exactly reported p-values.
AllP_exact <- subset(AllP, Reported.Comparison == "=") 

#Plot total reported p-values in range [.0-.10]. 
sum(AllP_exact$Reported.P.Value >= 0 & AllP_exact$Reported.P.Value <= .1) #n = 514
plot_aut_rep <- ggplot() + 
  geom_histogram(data = AllP_exact, aes(x = Reported.P.Value), fill = 'grey', colour='black', bins=10, binwidth = .01, boundary = 0, closed = "right") + #10 bins, width .01, black bin lines, bins colored grey, lines of bins are black, bins close on the right
  scale_x_continuous(expand = c(0, 0), breaks = seq(0, .10, by = .01), lim = c(0,.105)) + #ticks on the x-axis with intervals of .01         
  scale_y_continuous(expand = c(0, 0), breaks = seq(0,200, by = 25)) + #ticks on the y-axis with intervals of 25                               
  labs(title = "2A:  ‘APA’, total (*n* = 514)", x = "*p*-value") +  #add title and label to the x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines,  no gridline is chosen
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  theme(plot.title = ggtext::element_markdown(size = 12), axis.title.x = element_markdown()) #enabling markdown text in label x-axis, adapting title size

#Creating new data sets containing only reported results related to explicitly stated hypotheses and results not related to explicitly stated hypotheses, respectively.
AllP_exact_nh <- subset(AllP_exact, Result.Hypothesis == "Result not related to hypothesis") #data set containing only results of explicitly stated hypotheses
AllP_exact_h <- subset(AllP_exact, Result.Hypothesis == "Result related to hypothesis")  #data set containing only results not related to explicitly stated hypotheses

#Plot reported *p*-values in range [.0-.10] that are not related to explicitly stated hypotheses.
sum(AllP_exact_nh$Reported.P.Value >= 0 & AllP_exact_nh$Reported.P.Value <= .1) #n = 353
plot_aut_rep_nh <- ggplot() + 
  geom_histogram(data = AllP_exact_nh, aes(x = Reported.P.Value), fill = 'grey', colour='black', bins=10, binwidth = .01, boundary = 0, closed = "right") + #10 bins, width .01, black bin lines, bins colored grey, bins close on the left
  scale_x_continuous(expand = c(0, 0), breaks = seq(0, .10, by = .01), lim = c(0,.105)) + #ticks on the x-axis with equal intervals of .01     
  scale_y_continuous(expand = c(0, 0), breaks = seq(0,200, by = 25)) + #ticks on the y-axis with equal intervals of 25                                   
  labs(title = "2B: ‘APA’, no hypothesis (*n* = 353)", x = "*p*-value") +  #add title and label to the x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines, no gridline
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  theme(plot.title = ggtext::element_markdown(size = 12), axis.title.x = element_markdown()) #enabling markdown text in label x-axis, adapting title size

#Plot reported *p*-values in range [.0-.10] that are related to explicitly stated hypotheses.
sum(AllP_exact_h$Reported.P.Value >= 0 & AllP_exact_h$Reported.P.Value <= .1) #n = 161
plot_aut_rep_h <- ggplot() + 
  geom_histogram(data = AllP_exact_h, aes(x = Reported.P.Value), fill = 'grey', colour='black', bins=10, binwidth = .01, boundary = 0, closed = "right") + #10 bins, width .01, black bin lines, bins colored grey, bins close on the left
  scale_x_continuous(expand = c(0, 0), breaks = seq(0, .10, by = .01), lim = c(0,.105)) +     
    scale_y_continuous(expand = c(0, 0), breaks = seq(0,200, by = 25)) + #ticks on the y-axis with equal intervals of 25                                 
  labs(title = "2C: ‘APA’, result hypothesis (*n* = 161)", x = "*p*-value") +  #add title and label to the x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines, no gridline
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  theme(plot.title = ggtext::element_markdown(size = 12), axis.title.x = element_markdown()) #enabling markdown text in label x-axis, adapting title size


############################
##        Plot Hyp        ##  
############################

setwd("..") #setting working directory to 'thesis'
setwd("./Data") #setting working directory to 'Data'

#Loading in 'Hyp'.
Hyp <- read_excel("./Hyp & inclusion_Hyp/Hyp.xlsx", col_types = c(
#Article_numb, Article, Author(s), Journal
"numeric", "text", "text", "text", 
#Year, Issue, Number of hypotheses, Belongs to hypothesis
"numeric", "numeric", "numeric", "numeric", 
#Statistic, b, se, t_stat
"text", "numeric", "numeric", "numeric", 
#n, IVs, df_1,df_2
"numeric", "numeric",  "numeric", "numeric", 
#Reported.Comparison, Reported.P.Value, lb_b, ub_b 
"text", "numeric", "numeric", "numeric",
"numeric", "numeric", "numeric", "numeric", 
#dec_pval_rep, Computed, P_one_sided_chi, Error
"numeric", "numeric", "numeric", "numeric", 
#DecisionError, OneTailed, Marg.sig, Hypothesis
"numeric", "text", "numeric", "text", 
#Information from text article used, Comment
"text", "text"))

#Making '1E-3' and 'ns' numeric, such that 'Reported.P.Value' can be converted to a numeric variable properly.
Hyp$Reported.P.Value<- as.numeric(Hyp$Reported.P.Value)
#Subset exactly reported p-values in range [.0-.10].
Hyp_all_jrnl_exact <- subset(Hyp, Reported.Comparison == "=")
pb_bump_Hyp <- subset(Hyp_all_jrnl_exact, Reported.P.Value >= 0 & Reported.P.Value <= .1) #n = 167
#Keeping only the variable 'Reported.P.Value' for plotting purposes.
pb_bump_Hyp <- pb_bump_Hyp[ ,c("Reported.P.Value")]

#Creating the histogram.
plot_man_rep <- ggplot() + 
    geom_histogram(data = Hyp_all_jrnl_exact, aes(x = Reported.P.Value), fill = 'grey', colour='black', #10 bins of width .01, black bin lines, bins colored grey, bins close on the left
                 bins=10, binwidth = .01, boundary = 0, closed = "right") + #ticks on the x-axis with intervals of .01
  scale_x_continuous(expand = c(0, 0), breaks = seq(0, .10, by = .01), lim = c(0,.105)) + #ticks on the x-axis in the range [0, .105] with intervals of .01
  scale_y_continuous(expand = c(0, 0), breaks = seq(0, 200, by = 10)) + #ticks on the y-axis in the range [0, 200] +
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  labs(title = "2D: ‘Hyp’ (*n* = 167)", x = "*p*-value") +  #add title and label to the x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines and no gridline 
  theme(plot.title = ggtext::element_markdown(size = 12)) + #adapt title size
  theme(axis.title.x = ggtext::element_markdown()) #enabling markdown text in label x-axis

#Putting the plots created above in one figure. 
txt1 <- c("**Exactly reported *p*-values in the range [0, .10]**") #title of the figure
plot3_aut <- grid.arrange(plot_aut_rep, plot_aut_rep_nh, plot_aut_rep_h, plot_man_rep, ncol=2, nrow=2, top = richtext_grob(txt1,gp=gpar(fontsize=12, font=1))) #creating one Figure with four plots, and adding a title
plot_3lab_aut <- as_ggplot(plot3_aut) #putting created figure into ggplot format
```


```{r table 7, echo = FALSE, warning = FALSE, message = FALSE}
#Creating content table cells.
data_table7 <- c("", "", "", "",  "", "",
                 "10 (41.7%)", 14, 24, "17 (45.9%)", 20, 37, #begin AllP
                 "22 (44.9%)", 49, 43, "47 (52.2%)", 43, 90,
                 "", "", "", "", "", "",
                 "8 (40%)", 12, 20, "11 (37.9%)", 18, 29,
                 "3 (42.9%)", 4, 7, "8 (57.1%)", 6, 14,
                 "1 (20%)", 4, 5, "3 (42.9%)", 4, 7,
                 "14 (43.8%)", 18, 32, "28 (49.1%)", 29, 57,
                 "6 (66.7%)", 3, 9, "14 (70%)", 6, 20,
                 "", "", "", "", "", "", 
                 "13 (44.8%)", 16, 29, "27 (55.1%)", 22, 49,
                 "10 (52.6%)", 9, 19, "20 (52.6%)", 18, 38,
                 "9 (36%)", 16, 25, "17 (42.5%)", 16, 40,
                 "32 (43.8%)", 41, 73, "64 (50.4%)", 41, 127, #end AllP
                 "", "", "", "",  "", "",                     #begin Hyp
                 "2 (40%)", 3, 5, "5 (45.5%)", 6, 11,
                 "3 (100%)", 0, 3, "4 (80%)", 1, 5,
                 "4 (66.7%)", 2, 6, "5 (50%)", 5, 10,
                 "", "", "", "",  "", "",                     #begin Hyp
                 "4 (66.7%)", 2, 6, "8 (61.5%)", 5, 13,
                 "1 (50%)", 1, 2, "1 (50%)", 1, 2,
                 "4 (66.7%)", 2, 6, "5 (45.5%)", 6, 11,
                 "9 (64.3%)", 5, 14, "14 (53.8%)", 12, 26)    #end Hyp
#Putting content table cells into a matrix.
table7 <- matrix(data_table7, ncol = 6, nrow = 23, byrow = TRUE)

#Giving matrix row names and column names.
rownames(table7) <- paste(c("Relation to hypothesis", "Yes", "No", "Journal", 
                            "ASR", "AJS", "SQ", "JMF", "CHQ", "Year", 
                            "2014", "2015", "2016", "Total", "Journal", 
                            "ASR", "AJS", "SQ", "Year", 
                            "2014", "2015", "2016", "Total"))
colnames(table7) <- paste(c("(.04-.05]", "(.05-.06]", "Total", "(.03-.05]", "(.05-.07]", "Total"))

#Creating Table 7.
table7 <- table7 %>%
  #format latex, centered alignment, bold-faced caption
  kable(booktabs = T, align = "c", caption = "Descriptive statistics on publication bias for ‘AllP’ and ‘Hyp’.") %>% 
  #table will appear in correspondence with its place in this document, table is scaled-down to fit page size
  kable_styling(latex_options = c("striped", "HOLD_position", "scale_down"), full_width = F) 

table7 %>%
  #Adding indentation for the categories of all variables
  add_indent(c(2,3, 5:9, 11:13, 16:18, 20:22)) %>% 
  #grouping columns based on binwidth
  add_header_above(c(" ", "Binwidth .01" = 3, "Binwidth .02" = 3)) %>%
  #grouping rows for the two data sets
  pack_rows("‘AllP'", 1, 14) %>%
  pack_rows("‘Hyp'", 15, 23) %>%
  #fixing large spaces between columns binwidth .02
  column_spec(4:6, width = "5em") 
```
\pagebreak

# References

\begingroup
\noindent
\vspace{-2em}
\setlength{\parindent}{-0.4in}
\setlength{\leftskip}{0.4in}
\setlength{\parskip}{7pt}

<div id="refs"></div>.

\endgroup

\newpage

\beginsupplement

# Supplement

```{r table S1 hypotheses inconsistencies and gross inconsistencies, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#install.packages("knitr")
#library(knitr)
#devtools::install_version("knitr", "1.37")

#Creating content table cells.
hyp_table_err <- c("-1.820", ".157", "< .001", ".162 [.118, .218]", #intercept H1
                 "-.073", ".278", ".793", ".930 [.531, 1.585]", #relation hypothesis H1
                 "-4.422", ".503", "< .001", ".012 [.004, .028]", #intercept H2
                 ".708", ".714", ".321", "2.030 [.475, 8.685]") #relation hypothesis H2
#Putting content table cells into a matrix.
hyp_table_err <- matrix(hyp_table_err, ncol = 4, nrow = 4, byrow = TRUE)

#Giving matrix row and column names.
colnames(hyp_table_err) <- c("b", "SE", "p", "OR [95% CI]")
rownames(hyp_table_err) <- paste(c("Intercept", "Result hypothesis", "Intercept", "Result hypothesis"))

#Creating Table S1.
hyp_table_err %>%
  #format latex, centered alignment, bold-faced caption
kable(booktabs = T, format ="latex", align = "c", caption = "Logistic regressions for H1 and H2, which concern the difference in prevalence of (gross) inconsistencies in results related to hypotheses versus results not related to hypotheses.") %>% 
  #grouping data for the two hypotheses
  pack_rows("Inconsistencies (H1)", 1, 2) %>%
  pack_rows("Gross inconsistencies (H2)", 3, 4) %>%
  #table will appear in correspondence with its place in this document
  kable_styling(latex_options = c("HOLD_position", "striped"), full_width = F)
```
\hspace{20em}
\newline

```{r table S2 hypotheses publication bias, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
h3ab_table <- c("-.205", ".287", ".476", ".815 [.456, 1.428]",  #intercept H3a 
                 "-.132", ".504", ".794", ".877 [.321, 2.345]", #relation hypothesis H3a
                 ".089", ".211", ".673", "1.093 [.723, 1.658]", #intercept H3b
                 "-.251", ".392", ".521", ".778 [.358, 1.674]") #relation hypothesis Hb
#Putting content table cells into a matrix.
h3ab_table <- matrix(h3ab_table, ncol = 4, nrow = 4, byrow = TRUE)

#Giving matrix row and column names.
colnames(h3ab_table) <- c("b", "SE", "p", "OR [95% CI]")
rownames(h3ab_table) <- paste(c("Intercept", "Result hypothesis", "Intercept", "Result hypothesis"))

#Creating Table S2.
h3ab_table %>%
  #format latex, centered alignment, bold-faced caption
kable(booktabs = T, format ="latex", align = "c", caption = "Logistic regressions for H3a and H3b, which concerns the difference in prevalence of publication bias in results related to hypotheses versus results not related to hypotheses.") %>% 
  #table will appear in correspondence with its place in this document
  #grouping data for the two hypotheses
  pack_rows("Binwidth .01 (H3a)", 1, 2) %>%
  pack_rows("Binwidth .02 (H3b)", 3, 4) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), full_width = F)
```


