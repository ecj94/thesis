---
title: 'The reporting of statistical results in sociology: a systematic review'
author:
- 'Supervisor: Marcel van Assen'
date: '`r Sys.setlocale("LC_TIME", "English"); format(Sys.Date(), "%d %B %Y")`'
output:
  pdf_document:
    latex_engine: xelatex
    fig_caption: yes
    keep_tex: yes
  html_document:
    df_print: paged
  word_document: default
csl: apa7.csl #credits to bwiernik, see https://github.com/citation-style-language/styles/blob/master/apa.csl
bibliography: ref_thesis.bib
nocite: |
  @APA_list_journals, @APA_numb_stat_guide
header-includes:
- \usepackage{setspace}\doublespacing #use double spacing
- \newcommand{\beginsupplement}{\setcounter{table}{0} #counter for tables in supplement
- \renewcommand{\thetable}{S\arabic{table}} #giving Arabic numbers to tables
- \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}} #setting Arabic counter for figures 
- \usepackage{caption} #nice table and figure captions
- \captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline} #APA style captions tables
- \captionsetup[figure]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline} #APA style captions figures
- \usepackage{floatrow} #package that helps handling floating objects
- \floatsetup[figure]{capposition=top} #caption figure top
- \floatsetup[table]{capposition=top} #caption table top
- \usepackage{booktabs} #enhancement of table quality
- \usepackage{amssymb}  #latex features for (mathematical) symbols
- \usepackage{amsmath} #latex features for mathematical symbols
- \usepackage{longtable} #creating tables longer than one page
- \usepackage{multicol} #creating multiple column tables
- \usepackage{threeparttable} #adding structured footnotes to tables
- \setlength{\marginparwidth }{2cm} #setting margins to 2cm
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format = "latex") 
```
\pagenumbering{gobble}
\pagebreak

In this article, several aspects of statistical reporting in sociology were studied: the number of sociology journals that require adherence to the APA statistical reporting guidelines, the prevalence of statistical reporting errors, the presence of publication bias and a ‘bump’ in just significant *p*-values, and the prevalence of marginal significance among individual results and articles. Data were automatically retrieved from the 2014-2016 volumes of five sociology journals using the R package statcheck [@statcheck122]. Furthermore, data were retrieved manually from the 2014-2016 volumes of three sociology journals previously studied in research on publication bias by @Gerber2008. We found that only 13 of 143 sociology journals (9.1%) requested authors to adhere to the APA statistical reporting guidelines. Using statcheck, we found a slightly higher prevalence of inconsistent results and a prevalence of grossly inconsistent results comparable to previous studies in psychology [@Nuijten2016]. In our manually retrieved statistical results related to explicitly stated hypotheses, 3.5% of results were inconsistent and 0.5% grossly inconsistent. No convincing evidence of publication bias and a ‘bump’ in *p*-values was found. Assignment of marginal significance was rather prevalent, with 81.5% of manually retrieved results with *p*-values in the interval (.05-.10] labelled as such. Furthermore, in 43% (automatically retrieved data) and 63.3% (manually retrieved data related to hypotheses) of articles with *p*-values in this interval, marginal significance was assigned at least once. Implications of these results for statistical reporting quality in sociology are discussed. 
\pagenumbering{arabic} 
\

**Keywords**
statcheck, publication bias, statistical reporting errors, marginal significance, statistical reporting guidelines
\pagebreak

Statistical results in scientific articles provide scientific and nonscientific communities with essential information about studied phenomena. It is therefore vital that they live up to certain quality standards. Firstly, they should provide sufficient information for reproduction; this will make it easier for readers to critically assess the reported results of a study [@Simera]. Secondly, statistical results should not contain errors, because errors can lead to incorrect statistical conclusions, placing readers at risk of being misinformed about the nature of studied phenomena. Finally, the reporting of statistical results in articles should be standardized at least within disciplines to enable authors to clearly communicate them and readers to critically evaluate them. In this systematic review, we will examine several aspects of the quality of reporting of statistical results in sociology, namely  requested adherence to statistical reporting guidelines by journals, prevalence of statistical reporting errors, evidence of publication bias, evidence of a ‘bump’ in just significant *p*-values, and prevalence of *p*-values reported as marginally significant.\
&nbsp;&nbsp;&nbsp;&nbsp;It has been suggested that the presence of statistical reporting guidelines in a discipline may lead to less reporting errors [@Lang]. Their absence, on the other hand, leads to authors providing insufficient information when reporting statistics, making critical assessment of results difficult [@Simera]. Statistical reporting guidelines also increase comparability and communicability of statistical results in a discipline by providing one standard of reproducible reporting. Thus, adopting discipline-wide statistical reporting guidelines may well lead to better statistical reporting quality in a discipline. In psychology, the American Psychological Association (APA) manual and its accompanying statistical reporting guidelines serve this function to some extent. At present, they have been adopted in more than 90 psychology journals (APA, 2022b). Unfortunately, within sociology, no statistical reporting guidelines have been developed. Different sociology journals require authors to adhere to different style manuals, such as the APA, American Sociological Association (ASA), Chicago, Harvard, and Oxford style manuals. Of these manuals, only the APA manual contains statistical reporting guidelines. We examined which journals request authors to follow the APA manual to assess to what extent sociology journals require authors to adhere to statistical reporting guidelines.\
&nbsp;&nbsp;&nbsp;&nbsp;Statistical reporting errors, also called inconsistencies, occur when there is a discrepancy between the following parameters of a reported result: the test statistic, (if used) the degrees of freedom (*df*), and the *p*-value. Inconsistencies are undesirable because they reflect inaccuracies in reported results. Gross inconsistencies are inconsistencies that change statistical conclusions based on null hypothesis significance testing (NHST). This can cause audiences to inadvertently decide a true effect exists, or that it does not exist. An example of an inconsistency is ‘*t*(50) = 1.88, *p* = .056’, since *t*(50) = 1.88 implies *p* = .066. An example of a gross inconsistency is ‘*t*(50) = 1.99, *p* = .049’. This suggests a statistically significant result, but *t*(50) = 1.99 implies *p* = .052, which means H\textsubscript{0} should not be rejected. To our knowledge, no research on the prevalence of statistical reporting errors in sociology has previously been conducted. However, research in psychology found that 4.3%-12.8% of results are inconsistent [@Bakker2011; @Krawczyk; @Nuijten2016; @Veldkamp; @Vermeulen; @Wicherts]. Gross inconsistencies make up 0.8%-2.5% of reported results in psychology [@Bakker2011; @Nuijten2016; @Veldkamp; @Vermeulen], and occur relatively often in results which are reported as significant, but are in fact non-significant. @Nuijten2016 found that due to gross inconsistencies, there were 2.2 percentage points less significant recalculated *p*-values than significant reported *p*-values, while @Vermeulen found that 76.9% of gross inconsistencies were results erroneously reported as significant. Similarly, research in psychology found that 38.7%-67.45% of *p*-values reported as *p* = .05 had non-significant counterparts [@Leggett; @Hartgerink2016]. This could point to authors using the questionable research practice (QRP) of incorrect *p*-value rounding to obtain (false) significance [@John; @Hartgerink2016]. We studied the prevalence of statistical reporting errors in a selection of APA^[In this article, the term 'APA journal' does not refer to a sociology journal belonging to the APA organization. Instead, it refers to a journal that requires authors to follow the APA manual.] and non-APA sociology journals in two ways: manually, and with the R package statcheck [@statcheck122], which automatically checks the consistency of APA-reported results.\
&nbsp;&nbsp;&nbsp;&nbsp;Publication bias occurs when statistically non-significant results are published relatively less often than significant ones [@Dickersin]. It has been found in various scientific fields [e.g., @Dickersin; @Easterbrook; @Fanelli2010; @Fanelli2011; @Franco2014; @Kuhberger; @Lakens2015a in his reanalysis of De Winter & Dodou, 2015; @Franco2016]. Publication bias is caused by the selection of articles for publication based on significant results [@Maxwell; @Song] combined with the dependence of scientists' career success on publishing articles with significant results [@Dickersin; @Lawrence; @Song]. This mechanism increases the relative amount of published significant results by reducing the amount of published non-significant results [@Hartgerink2016]. Publication bias is problematic because an overrepresentation of significant results limits the scientific community's ability to nuance or correct previous findings [@Knight]. Thereby, it might inhibit scientific progress. Potential causes of a high prevalence of significant results are QRPs that lead to *p*-hacking, like rounding down *p*-values such that they become significant or conducting multiple statistical analyses and only reporting the lowest obtained *p*-value [@Hartgerink2016; @John; @Ulrich]. In various scientific fields, publication bias has been indicated by a low prevalence of just nonsignificant *p*-values relative to just significant ones [@Ginsel; @DeWinter]. In psychology, @Lakens2015b found evidence of publication bias in his reanalysis of @Masicampo, and @Kuhberger found three times more just significant results than just nonsignificant ones in 531 psychology articles using caliper tests. In sociology, @Gerber2008 found evidence of publication bias among results corresponding to hypotheses from the 2003-2005 volumes of American Sociological Review (*ASR*), American Journal of Sociology (*AJS*) and Sociological Quarterly (*SQ*). Like @Kuhberger, they compared numbers of just significant *z*-values to numbers of just non-significant *z*-values using caliper tests and found that the number of just significant *z*-values was 2.4 to 4 times higher than that of just nonsignificant *z*-values. Following @Masicampo and @DeWinter, we studied the *p*-value intervals (.04-.05] and (.05-.06]. Additionally, we studied the *p*-value intervals (.03-.05] and (.05-.07], since larger intervals provide higher power. We studied *p*-values rather than *z*-values because in this way, we could include results of a wider range of statistical analyses.\
&nbsp;&nbsp;&nbsp;&nbsp;A ‘bump’ in *p*-values occurs when there are more *p*-values in a just statistically significant *p*-value interval than in the adjacent lower *p*-value interval. According to @Hartgerink2016, it is evidence of specific QRPs that can lead to left-skewedness in the distribution of significant *p*-values, for instance: exclusion of outliers after having conducted analyses [@Bakker2014], or using a different sample of data if the previous one(s) did not provide significant results - also called data peeking, see @Armitage. Across a variety of disciplines, @Head2015 found indications of a 'bump' in the *p*-value intervals (.045-.05) and (.025-.05). However, @Hartgerink2017 found no evidence of a 'bump' when reanalyzing this study's data with different binwidths. In psychology, some studies focusing on the *p*-value interval (.04-.05] also claimed to have found an overrepresentation of just significant *p*-values [@Hartgerink2016; @Masicampo; @Leggett]. However, a reanalysis by @Lakens2015b of @Masicampo showed that the overrepresentation of just significant *p*-values they found for one specific binwidth was likely coincidental. Furthermore, @Simonsohn and @Hartgerink2016 argued that data peeking does not result in a ‘bump’ if true effect sizes are medium (Cohen’s *d* = 0.5) or larger. Although this implies that the absence of a ‘bump’ is no evidence of absence of QRPs, the presence of a ‘bump’ can only be explained by QRPs such as those mentioned above. Following @Hartgerink2016, we studied the presence of a ‘bump’ using the *p*-value intervals (.04-.05] versus (.03-.04] and (.03-.05] versus (.01-.03]. Larger intervals were again used because they may provide higher testing power, although power may also decrease because *p*-values near .01 will be more prevalent than *p*-values near .05 in case of true nonzero effects [@Hartgerink2016].\
&nbsp;&nbsp;&nbsp;&nbsp;We also examined the prevalence of results reported as marginally significant in sociology. The reporting of marginally significant results occurs when authors argue that statistically non-significant results (*p* > .05) provide evidence of nonzero true effects, although one can argue they have low evidential value [@Benjamin; @OhlssonCollentine; @Pritschet]. Thus, assigning marginal significance may result in (unwarranted) false positives. Since this can lead to audiences assuming a true effect exists while evidence for it is slight, marginally significant *p*-values can be considered undesirable. *P*-values reported as marginally significant can mainly be found in the interval (.05-.10]; according to @Pritschet, 92.6% of *p*-values reported as marginally significant in psychology were found here. @OhlssonCollentine found that almost 40% of *p*-values in the (.05-.10] interval retrieved from 44,200 psychology articles published between 1985-2016 were reported as marginally significant. They also found that almost 20% of articles containing *p*-values had at least one *p*-value in the (.05-.10] interval that was reported as marginally significant. In addition, @Pritschet found that 18% of articles from three psychology journals published in 1970 assigned marginal significance at least once, and that in 2010, this had increased to 54%. In sociology, @Leahey found that 10% of a stratified random sample of *ASR* and *AJS* articles published between 1995-2000 used a significance level of low evidential value of *p* < .10. We followed @OhlssonCollentine by studying the prevalence of assignment of marginal significance to results within the *p*-value interval (.05-.10]. Also, we examined the percentage of articles with *p*-values in the interval (.05-.01] in which marginal significance was assigned at least once to a *p*-value in this interval.\
&nbsp;&nbsp;&nbsp;&nbsp;We studied statistical reporting errors, publication bias, a ‘bump’ in *p*-values, and *p*-values reported as marginally significant among results of explicitly stated hypotheses (hypotheses referred to in the article’s text as hypotheses to be tested) and results not related to explicitly stated hypotheses. We formally tested whether there were differences in the prevalence of three of these topics - statistical reporting errors, publication bias, and assignment of marginal significance - between results of explicitly stated hypotheses and other results. One would hope that at least reported results of explicitly stated hypotheses would be without inaccuracies through careful checking by authors before submission and by reviewers and editors before accepting an article. On the other hand, publication bias has been assumed to primarily operate on results related to hypotheses [see @Gerber2006]. This may also be hypothesized for statistical reporting errors and marginal significance. Statistical reporting errors may be more prevalent among results related to hypotheses if publication bias and its accompanying pressure to publish positive results lead to QRPs such as (accidentally or intentionally) rounding down *p*-values incorrectly or adding extra zeroes (e.g., turning *p* = 0.13 into *p* = 0.013) for variables key to testing hypotheses. Assigning marginal significance may also be more prevalent in results related to hypotheses, since it allows authors to try to convince readers that there is reason to assume a proposed effect central to the article's hypotheses is a true effect, even though it is non-significant. Therefore, we expected the prevalence of (gross) inconsistencies, publication bias, and marginal significance to be higher among results corresponding to explicitly stated hypotheses. More specifically, we hypothesized the following:
\

*H1: The prevalence of statistical reporting errors is higher among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses.*
\

*H2: The prevalence of gross statistical reporting errors is higher among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses.*
\

*H3a: The discrepancy between the amounts of p-values in the intervals (.04-.05] and (.05-.06] is larger among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses.*
\

*H3b: The discrepancy between the amounts of p-values in the intervals (.03-.05] and (.05-.07] is larger among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses.*
\

*H4: The prevalence of p-values in the interval (.05-.10] reported as marginally significant is higher among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses.*
\

We did not construct similar hypotheses for a possible ‘bump’ in *p*-values. Due to sample sizes generally being larger in sociology than in psychology, statistical power has been suggested to be higher in sociology [@Sedlmeier; @Cohen]. Assuming the same distribution of examined true effects in both fields, higher statistical power implies lower *p*-values on average in sociology. Therefore, we expected neither a ‘bump’ in *p*-values in sociology in general, nor a difference in the presence or size of a ‘bump’ between results related to hypotheses and results not related to hypotheses.

\vspace{2cm}

# Method
## Data sources 
For our study on statistical reporting guidelines, we consulted @WOS to create a data set of sociology journals called 'SRG' ('Statistical Reporting Guidelines'). For each journal in 'SRG', we verified whether it requested adherence to the APA manual - and thus, to its statistical reporting guidelines - or not.\
&nbsp;&nbsp;&nbsp;&nbsp;To study statistical reporting errors, publication bias, the ‘bump’ in *p*-values, and marginal significance, we collected data from articles of several journals. Since statcheck only recalculates APA-reported results, we collected articles from two sociology journals from @WOS that require APA statistical reporting: Cornell Hospitality Quarterly (*CHQ*) and Journal of Marriage and Family (*JMF*). Of sociology journals requiring APA statistical reporting, these were the ones with the highest impact factors from which statcheck could extract results (*CHQ* ranked first with 2.657, *JMF* third with 2.238)^[Initially, we had collected articles from *CHQ* and Work and Occupations (*WOX*), which had the second highest impact factor (2.355). However, for an unknown reason, no results could be extracted by statcheck from neither the HTML nor PDF versions of *WOX* articles.]. We examined all 310 articles from the 2014-2016 volumes of these journals. To compare the prevalence of statistical reporting errors in APA journals and non-APA journals, we also examined results from the 322 articles of the 2014-2016 volumes of three non-APA journals from @WOS: *ASR*, *AJS*, and *SQ*. @Gerber2008 used three volumes of these journals in their study on publication bias in sociology, which we wanted to conceptually replicate. APA-reported results retrieved by statcheck were put into a data set called ‘APA’, and all *p*-values retrieved by statcheck (APA-reported or not) were put into a data set called ‘AllP’, implying that 'APA' is a subset of 'AllP'. Finally, we created a data set called ‘Hyp’, which contains all manually retrieved *p*-values and statistical results related to explicitly stated hypotheses from *ASR*, *AJS*, and *SQ*. Thus, some *p*-values and APA-reported results are also included in ‘Hyp’, as 'Hyp' contains all statistical results related to explicitly stated hypotheses. 
\

## Data collection^[R code used to create the data sets on results-related topics is available at https://github.com/ecj94/thesis/tree/development/Code]
For each sociology journal in @WOS, we verified if it explicitly required authors to adhere to the APA, ASA, Chicago and/or Harvard style guide and/or another external style guide. We also examined if journals explicitly required authors to follow their own journal’s style guide, and if they allowed authors to follow different style guides. This information was put into data set 'SRG' ^['SRG' is available at https://github.com/ecj94/thesis/tree/development/Data/SRG]. There was explicitly required adherence to the own journal’s guidelines if one of the following expressions was found on the journal’s website: 1) ‘House style (guide) *X*’ or ‘Journal style (guide) *X*’, where *X* represents the journal’s name, or 2) ‘*X* (format) requirements’ or ‘*X* (format) requirements’, where again *X* represents the journal’s name. If some form of style guidelines was available, but there was no explicitly named style guide, a journal was put into the category ‘Other’.\
&nbsp;&nbsp;&nbsp;&nbsp;Before extracting statistical data with statcheck, we converted all relevant articles to HTML format. Statcheck namely converts HTML or PDF files to plain text before extracting statistics, and conversion from HTML format is accompanied by less errors [@Nuijten2016]. We then applied statcheck’s ‘checkHTMLdir’ function to a folder with HTML files to automatically obtain APA-reported results and recalculated *p*-values. Data set ‘APA’ contains information retrieved by statcheck on all aspects of APA-reported results from all five journals: test statistics (*t*, *z*, *F*, $\chi^2$, and *r*), *df*, and *p*-values reported using '=', ‘<’, ‘>’, or ‘non-significant'. If *p*-values were reported as non-significant, statcheck assigned them the label ‘NA’. 'APA' also contains *p*-values recalculated by statcheck, and information from statcheck on whether reported results are (grossly) inconsistent with their recalculated counterparts ^['APA' is available at https://github.com/ecj94/thesis/tree/development/Data/APA]. If a reported result seemed inconsistent (and this could not be due to correct rounding), statcheck applied a one-sided test to it. If this led to a consistent reported result, statcheck kept the one-sided test. Otherwise, it kept the two-sided test [@Nuijten2017]. We also manually put the part of the article’s text from which we concluded that a result was (not) related to an explicitly stated hypothesis in a separate column. Our definition of explicitly stated hypotheses followed that of @Gerber2008, i.e., hypotheses were considered explicitly stated if they were bolded, italicized, or indented, or if they were listed using one of the following terminologies: ‘Hypothesis 1’, ‘H1', ‘H~1~', or ‘the first hypothesis’.\
&nbsp;&nbsp;&nbsp;&nbsp;Data set 'APA' was used to test our hypotheses on statistical reporting errors (H1 and H2). Of 524 retrieved statistical results, we removed 19 (3.6%) because they did not refer to APA-reported results. In total, 505 statistical results from 76 articles were used in descriptive analyses and hypothesis testing (see Table 1 and Table 2). \


```{r Table 1 overview information data sets, echo = FALSE, warning = FALSE, message = FALSE}
library('dplyr') #using pipes
library('knitr') #generating document
library('kableExtra') #making a nice table
library('tidyverse') 

#Creating content table cells.
data_table1 <- c("all", "all", "\\emph{ASR}/\\emph{AJS}/\\emph{SQ}", #journals
                 "text", "text", "text/table/figure", #part(s) of article from which info was retraced
                 "partly", "partly", "yes", #results related to explicitly stated hypotheses?
                 471, 80, 91, #total number of articles
                 314, 76, 91, #number of relevant articles
                 "7,280", 524, "4,929", #total number of results
                 "2,960", 505, "4,929") #number of relevant results
#Putting content table cells into a matrix.
table1 <- matrix(data_table1, ncol = 3, nrow = 7, byrow = TRUE)

#Giving matrix row and column names.
rownames(table1) <- paste(c("Journals", "Part(s) of article from which info was retrieved", "Statistical results related to hypotheses?", "Total number of articles", "Number of articles used", "Total number of statistical results", "Number of valid statistical results"))
colnames(table1) <- paste(c("‘AllP’", "‘APA’", "‘Hyp’"))

#Creating Table 1.
table1 %>%
  #format latex, centered alignment, caption
  kable(booktabs = T, align = "c", caption = "Overview of information provided by ‘AllP', ‘APA', and ‘Hyp'.", escape = FALSE) %>% #booktabs used, centered alignment, caption
  kable_styling(latex_options = "HOLD_position", full_width = F)
```

```{r Table 2 amount of data results and article level topics, echo = FALSE, warning = FALSE, message = FALSE}
#Creating content table cells.
data_table2 <- c("-", "505 (76)", "404 (19)", #descriptive information
                 "-", "505 (76)", "-", #H1 & H2
                 "", "", "",    #blank row
                 "73 (50)", "-", "14 (7)",    #descriptives publication bias binwidth .01
                 "127 (71)", "-", "26 (11)",   #descriptives publication bias binwidth .02
                 "", "", "",    #blank row
                 "73 (50)", "-", "-",  #H3a
                 "127 (71)", "-", "-", #H3b
                 "", "", "",    #blank row
                 "64 (40)", "-", "14 (7)",    #descriptives bump in p-values binwidth .01
                 "184 (80)", "-", "37 (12)",   #descriptives bump in p-values binwidth .02
                 "199 (107)", "-", "130 (30)", #descriptives marginal significance
                 "199 (107)", "-", "-")        #H4
#Putting content table cells into a matrix.
table2 <- matrix(data_table2, ncol = 3, nrow = 13, byrow = TRUE)

#Giving matrix row and column names.
rownames(table2) <- paste(c("Descriptive information", "Testing hypotheses (gross) inconsistencies (H1 & H2)", #statistical reporting errors
                            "Descriptive information", "(.04-.05] - (.05-.06]", "(.03-.05] - (.05-.07]", "Testing hypotheses publication bias", "H3a: (.04-.05] - (.05-.06]", "H3b: (.03-.05] - (.05-.07]", #publication bias
                            "Descriptive information", "(.03-.04] - (.04-.05]", "(.01-.03] - (.03-.05]", #bump in p-values
                            "Descriptive information", "Testing hypothesis marginal significance (H4)")) #marginal significance
colnames(table2) <- paste(c("‘AllP'", "‘APA'", "‘Hyp'"))


#Creating Table 2.
table2 <- table2 %>%
  #booktabs used, centered alignment, caption
  kable(booktabs = T, align = "c", caption = "Overview of numbers of statistical results and accompanying articles used in analyses of results-related topics for ‘AllP', ‘APA', and ‘Hyp'.") %>%  
  #table will appear in correspondence with its place in this document
  kable_styling(latex_options = c("striped", "HOLD_position"), font_size = 12, full_width = F) 

table2 %>%
  #adding indentation for different parts of topics
  add_indent(c(4, 5, 7, 8, 10, 11)) %>%
  #grouping data for the different topics
  pack_rows("Statistical reporting errors", 1, 2) %>%
  pack_rows("Publication bias", 3, 8) %>%
  pack_rows("Bump in \\emph{p}-values", 9, 11, escape = FALSE) %>%
  pack_rows("Marginal significance", 12, 13) %>%
  #adding footnote
  add_footnote("\\emph{Note}. Numbers of articles from which statistical results were used in analyses are shown", notation = "none", escape = F) %>%
  add_footnote("between parentheses.", notation = "none")
```
\

&nbsp;&nbsp;&nbsp;&nbsp;The third data set, ‘AllP’, consists of all reported *p*-values retrieved by statcheck from all five journals^['AllP' is available at https://github.com/ecj94/thesis/tree/development/Data/AllP]. We manually added information on whether reported *p*-values were related to an explicitly stated hypothesis as we did for ‘APA’. Of 7,280 results retrieved by statcheck from 471 articles, we removed 4,320 (59.3%) because they did not refer to reported *p*-values^[The removed results corresponded to, for instance, duplicates of *p*-values that had already been extracted, and *p*-values that were not related to a specific result but were mentioned in the article to indicate which significance levels were used. There was also one case in which 'Ns' was extracted while meaning 'numbers' instead of 'non-significant'.]. After this, 'AllP' contained 2,960 reported *p*-values from 314 articles  (see Table 1). Using these data, descriptive information on publication bias, the ‘bump’ in *p*-values, and assignment of marginal significance was obtained, and H3a, H3b, and H4 were tested (see Table 2).\
&nbsp;&nbsp;&nbsp;&nbsp;To determine if marginal significance was assigned to a reported *p*-value, we looked up *p*-values in the (.05-.10] interval in the text of articles. Then, following @Pritschet and @OhlssonCollentine, we decided that a *p*-value from 'AllP' was assigned marginal significance by authors if the expressions ‘margin\*’ or ‘approach\*’ were mentioned in relation to its significance. The text used to conclude that a *p*-value was (not) assigned marginal significance was stored manually in a separate column of ‘AllP’. 
We also obtained descriptive statistics on how many articles with *p*-values in the interval (.05-.10] had assigned marginal significance at least once to such *p*-value. This is different from the approach of @Pritschet and @OhlssonCollentine, who studied what percentage of articles with any reported *p*-values assigned marginal significance at least once to a *p*-value in the interval (.05-.10]. We prefer our approach because it enables us to show how likely sociology authors reporting *p*-values in the interval (.05-.10] are to assign marginal significance while this is likely unwarranted.\
&nbsp;&nbsp;&nbsp;&nbsp;A fourth data set, ‘Hyp’, was created to conceptually replicate the study of @Gerber2008 on publication bias by manually retrieving results from articles^['Hyp' is available at https://github.com/ecj94/thesis/tree/development/Data/Hyp]. Manual retrieval allows one to retrieve information from tables, figures, and text, whereas statcheck can only retrieve information from text. We only collected data from articles that met our inclusion criteria. Like @Gerber2008, we only studied articles that explicitly stated one or more hypotheses before their results were presented. Of the 322 articles from *ASR*, *AJS* and *SQ*, 99 (30.7%) met this criterion. Furthermore, articles had to contain at least one 'required statistic', i.e., at least one *p*-value or reproducible result related to an explicitly stated hypothesis. This was the case for 91 articles (28.3%) (see Figure 1 for an overview of the selection process)^[The data set used to select articles for 'Hyp' is available at https://github.com/ecj94/thesis/tree/development/Data/Hyp]. Following @Gerber2008, ‘Hyp’ contains all statistics from all models that were essential to testing explicitly stated hypotheses. Since results corresponding to control variables are not the focal point in hypothesis testing, they were not included in the data set. Whether individual statistics were essential for hypothesis testing or not was determined by reading the article's hypotheses and the part(s) of the article's text in which the results of hypothesis testing were discussed. Furthermore, following @Gerber2008, information from appendices was also included, but information from supplements was not, since only appendices are part of articles as published.\

<center>

![Flowchart describing the process of selecting articles from which results were retrieved for ‘Hyp’.](Flowchart_thesis.png)

</center>

&nbsp;&nbsp;&nbsp;&nbsp;There are some differences between our study on publication bias and that of @Gerber2008. @Gerber2008 performed caliper tests on *z*-values and *t*-values (converted to *z*-values) within 5%, 10%, 15%, or 20% of *z* = 1.64 (one-sided testing) and *z* = 1.96 (two-sided testing). If *z*-values or *t*-values were unavailable, regression coefficients and standard errors were used to calculate *z*-values. We used only exactly reported (not recalculated) *p*-values in the intervals (.04-.06] and (.03-.07] to study publication bias instead.^[It should be noted that these *p*-value intervals largely overlap with the 5% and 10% caliper tests of @Gerber2008. E.g., for two-sided tests, the 5% caliper had an equivalent *p*-value interval of (.039-.063), while the 10% caliper had an equivalent *p*-value interval of (.031-.077\].] We did this because it was often unknown what kind of distribution an analysis was based on, and because it allowed us to also include *p*-values based on *F*-values, *r*-values, and $\chi^2$-values. We did not mix reported and recalculated *p*-values in our analyses because @Krawczyk and @Hartgerink2016 found that there can be differences in reported and recalculated *p*-value distributions around *p* = .05, and @Hartgerink2016 have argued that these two types of *p*-values should therefore not be mixed in studies on reporting practices. Finally, @Gerber2008 excluded articles with more than 38 relevant coefficients because their inclusion could have a disproportionate impact on analyses. We did not do so since we wanted to include all *p*-values relevant for studying publication bias.\
&nbsp;&nbsp;&nbsp;&nbsp;We organized all available aspects of a result of an explicitly stated hypothesis - *p*-value, regression coefficient (or odds ratio, proportional hazard, etc.), *z*-value, *t*-value, *F*-value, *r*-value, $\chi^2$-value, standard error, sample size, *df*, phrasing of the hypothesis a result belonged to as retrieved from the article, and, if applicable, text from the article in which a result was mentioned - as we did for ‘APA’. In total, ‘Hyp’ contained 4,929 results (see Table 1). It was used to study all our results-related phenomena of interest (see Table 2). Where possible, we checked whether statistical results were (grossly) inconsistent by recalculating their *p*-values using the following functions from the R stats package [@Rstats]: pt() for *t*-values and *r*-values^[In order to use the pt() function on *r*-values, *r*-values first had to be converted to *t*-values with the formula: $$ t = \frac{r\sqrt{df}}{\sqrt{1-r^2}} $$ where *df* = *n* - 2.], pnorm() for *z*-values, pf() for *F*-values, and pchisq() for $\chi^2$-values. For detailed information on how this was done, see Table 3 and Table 4. We also manually added information to ‘Hyp’ on assignment of marginal significance to in-text *p*-values in the (.05-.10] interval as we did for ‘AllP’. For *p*-values in captions of tables and figures, we considered significance levels of *p* < .10 (indicated by, e.g., an asterisk) to be assignment of marginal significance. Finally, among articles in ‘Hyp’ with *p*-values in the interval (.05-.10], we studied the percentage of articles in which marginal significance was assigned at least once to a *p*-value in this interval. Note that ‘AllP’, ‘APA’, and ‘Hyp’ overlap. For instance, an in-text APA-reported result related to an explicitly stated hypothesis is included in all three data sets.
\

```{r Table 3 definition consistencies and gross inconsistencies Hyp, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating contents table cells.
data_table3 <- c("ns", "Cal\\emph{P} $\\leq$ .05", "Cal\\emph{P} $\\leq$ .05", #nonsignificant results
                 "$<$", "Cal\\emph{P} $\\geq$ Rep\\emph{P}", "Cal\\emph{P} $>$ .05 \\& Rep\\emph{P} $\\leq$ .05", #results with '<'
                 "$\\geq$", "Cal\\emph{P} $<$ Rep\\emph{P}", "Cal\\emph{P} $<$ .05 \\& Rep\\emph{P} $\\geq$ .05", #results with '≥'
                 "=", " Cal\\emph{P} $\\neq$ Rep\\emph{P} not due to", "Cal\\emph{P} $\\neq$ Rep\\emph{P} not due to rounding,", "", "rounding\\textsuperscript{*}", "Cal\\emph{P} $\\leq$ .05 \\& Rep\\emph{P} $>$ .05 or vice versa") #results with '-'
#Putting content table cells into a matrix.
table3 <- matrix(data_table3, ncol = 3, nrow = 5, byrow = TRUE)
#Giving matrix column names.
colnames(table3) <- paste(c("Type of Rep\\emph{P}", "Inconsistent if...", "Grossly inconsistent if..."))

#Creating Table 3.
table3 %>%
  #booktabs used, centered alignment, caption
  kable(booktabs = T,  align = "c", caption = "Inconsistencies and gross inconsistencies as determined for different types of reported p-values in  ‘Hyp'.", escape = FALSE) %>% 
  #table will appear in correspondence with its place in this document
  kable_styling(latex_options = c("HOLD_position"), full_width = F) %>% 
  #Adding footnotes
  add_footnote("\\emph{Note}. Cal\\emph{P} = recalculated \\emph{p}-value, Rep\\emph{P} = reported \\emph{p}-value, ns = non-significant", notation = "none", escape = FALSE)  %>%
  add_footnote("See Table 4 for methods used to determine whether a difference between recalculated and", notation = "symbol", escape = FALSE) %>%
  add_footnote(" reported \\emph{p}-values could be due to correct rounding or not.", notation = "none", escape = FALSE)  %>%
  #specifying font size
  kable_styling(font_size = 12)
```
\pagebreak

```{r Table 4 difference exactly reported and recalculated p-values (not) due to rounding, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
data_table4 <- c(
##b & SE
"Only used for recalculation if a result was explicitly based on the \\emph{z}-distribution or", "\\emph{t}-distribution. Take, e.g., \\emph{b} $=$ 3.11, SE $=$ 2.11, \\emph{p} $=$ 0.07, from a \\emph{z}-distribution:", #intro b&SE
                 
"- Correct Cal\\emph{P} stem from \\emph{b}\\textsubscript{lb} $\\leq$ \\emph{b} $<$ \\emph{b}\\textsubscript{ub} (e.g., 3.105 $\\leq$ \\emph{b} $<$ 3.115) and" , "SE\\textsubscript{lb} $\\leq$ SE $<$ SE\\textsubscript{ub} (e.g., 2.105 $\\leq$ SE $<$ 2.115).", #correct intervals
                 
"- Calculate \\emph{t}/\\emph{z}\\textsubscript{ub} $=$ $\\frac{\\emph{b} \\textsubscript{ub}}{SE\\textsubscript{lb}}$ and \\emph{t}/\\emph{z}\\textsubscript{lb} $=$ $\\frac{\\emph{b} \\textsubscript{lb}}{SE\\textsubscript{ub}}$, the largest and smallest \\emph{t}/\\emph{z} consistent", #calculation t/z
                 "with \\emph{b} and SE (e.g., \\emph{z}\\textsubscript{ub} $=$ $\\frac{3.115}{2.105}$ $=$ 1.47981 and \\emph{z}\\textsubscript{lb} $=$ $\\frac{3.105}{2.115}$ $=$ 1.468085).", #example calculation t/z

"- Calculate \\emph{t}/\\emph{z}\\textsubscript{lb} $=$ Cal\\emph{P}\\textsubscript{ub} and \\emph{t}/\\emph{z}\\textsubscript{ub} = Cal\\emph{P}\\textsubscript{lb}, i.e., boundaries of correctly", " rounded Rep\\emph{P}. For this, the R stats package pt() function (for \\emph{t}) or the pnorm()", "function (for \\emph{z}) is used.", 
"- Round Cal\\emph{P}\\textsubscript{lb} and Cal\\emph{P}\\textsubscript{ub} to the same number of decimals as Rep\\emph{P} with", #calculation Callb and Calub
                  "R base round() function. In our example, Cal\\emph{P}\\textsubscript{lb} $\\approx$ 0.07 and Cal\\emph{P}\\textsubscript{ub} $\\approx$ 0.07.", #example calculation Callb and Calub
"- If Cal\\emph{P}\\textsubscript{lb} $\\leq$ Rep\\emph{P} $\\leq$ Cal\\emph{P}\\textsubscript{ub}, Rep\\emph{P} is considered correct. This is the case in our", "example.", #conclusion

"", #white space row

##test statistics
"Functions of the R stats package used to recalculate \\emph{p}-values: pt() for \\emph{t} and \\emph{r},", "pnorm() for \\emph{z}, pf() for \\emph{F}, and chisq() for $\\chi^2$. All functions, except pnorm(), require \\emph{df}.", #intro
                 "We use the example of \\emph{t}(61) $=$ 3.11, \\emph{p} $=$ 0.0001:", #intro example

"- Correct Cal\\emph{P} stem from \\emph{t}\\textsubscript{lb} $\\leq$ \\emph{t} $<$ \\emph{t}\\textsubscript{ub} (e.g., 3.105 $\\leq$ \\emph{t} $<$ 3.115).", #correct intervals

"-	Calculate Cal\\emph{P}\\textsubscript{lb} and Cal\\emph{P}\\textsubscript{ub}, i.e., the \\emph{p}-values consistent with the highest and", "lowest \\emph{t}-values possible under correct rounding, using pt() function.", #calculation Callb & Calub
"- Round Cal\\emph{P}\\textsubscript{lb} and Cal\\emph{P}\\textsubscript{ub} to the same number of decimals as Rep\\emph{P} with R base", 
                 " round() function. In our example, Cal\\emph{P}\\textsubscript{lb} $\\approx$ 0.001 and Cal\\emph{P}\\textsubscript{ub} $\\approx$ 0.001.", #rounding Callb & Calub 
"- If Cal\\emph{P}\\textsubscript{lb} $\\leq$ Rep\\emph{P} $\\leq$ Cal\\emph{P}\\textsubscript{ub}, Rep\\emph{P} is considered correct. This is not the case in", #conclusion
                 "our example, since Rep\\emph{P} $<$ Cal\\emph{P}\\textsubscript{lb} $<$ Cal\\emph{P}\\textsubscript{ub}." #conclusion example
                 )

#Putting content table cells into a matrix.
table4 <- matrix(data_table4, ncol = 1, nrow = 24, byrow = TRUE)

#Creating Table 4.
table4 %>%
  #format latex, alignment to the left, caption
  kable(booktabs = T,  align = "l", longtable = T, caption = "Ways of determining whether discrepancies between exactly reported p-values and their recalculated counterparts from  ‘Hyp' could be due to correct rounding or indicate an inconsistency.", escape = FALSE) %>% 
  #grouping data for the different methods of recalculation
  pack_rows("b & SE", 1,14) %>%
  pack_rows("test statistics", 15, 24) %>%
  #Adding indentation 
  add_indent(c(3:14, 18:24)) %>% 
  #table will appear in correspondence with its place in this document, table is scaled-down to fit page size, font size is 12
  kable_styling(latex_options = c("HOLD_position", "repeat_header"), full_width = F) %>%
  kable_styling(font_size = 12) %>% 
  #adding footnotes with small font size
  footnote(general = "\\\\emph{b}\\\\textsubscript{ub} = upper bound \\\\emph{b}, SE\\\\textsubscript{lb} = lower bound SE, SE\\\\textsubscript{ub} = upper bound SE.", escape = FALSE, fixed_small_size = T, footnote_as_chunk = T, general_title = "") %>%
  footnote(general = "recalculated \\\\emph{p}-value, Cal\\\\emph{P}\\\\textsubscript{ub} = upper bound recalculated \\\\emph{p}-value, \\\\emph{b}\\\\textsubscript{lb} = lower bound \\\\emph{b}", escape = FALSE, fixed_small_size = T, footnote_as_chunk = T, general_title = "") %>% 
  footnote(general = "\\\\emph{Note}. Rep\\\\emph{P} = reported \\\\emph{p}-value, Cal\\\\emph{P} = recalculated \\\\emph{p}-value, Cal\\\\emph{P}\\\\textsubscript{lb} = lower bound", general_title = "", escape = FALSE, fixed_small_size = T, footnote_as_chunk = T) 
```



## Statistical analyses
In our descriptive analyses (which consist of frequencies and percentages), we reported how many journals from 'SRG' require authors to adhere to the APA statistical reporting guidelines. For (gross) inconsistencies, descriptive statistics were based on reproducible results from ‘APA’ and ‘Hyp’. We followed @Vermeulen and @Nuijten2016 by studying the direction of gross inconsistencies: do errors make non-significant results significant, or vice versa? For publication bias and the 'bump' in *p*-values, descriptive results were based on exactly reported *p*-values from ‘AllP’ and ‘Hyp’. Inexactly reported *p*-values were excluded for these topics because, as @Hartgerink2016 has shown for a 'bump' in *p*-values, they can lead to 'spikes' in certain *p*-values (e.g., including *p* < .05 will likely lead to a spike at *p* = .05). Data sets 'AllP' and 'Hyp' also provided descriptive statistics at the results and article level for marginal significance. For all topics but statistical reporting guidelines, descriptive results were split by explicitly stated hypothesis (yes/no), journal (*ASR*, *AJS*, *SQ*, and, for results from 'APA' and 'AllP', *CHQ*, and *JMF*), and year (2014-2016).   
&nbsp;&nbsp;&nbsp;&nbsp;@Nuijten2017 have argued that the prevalence of (gross) inconsistencies can be studied in three ways. Firstly, one can calculate the percentages of inconsistencies and gross inconsistencies for each article and take the averages of these percentages over all articles. Secondly, one can calculate the overall percentage of (gross) inconsistencies by dividing the amount of (gross) inconsistencies by the total number of obtained reported results. This is what we have done in our descriptive analyses. Finally, @Nuijten2017 described that one can use multilevel logistic regression models to estimate the probability that a reported result is inconsistent, while controlling for the nesting of results within articles. Although this method is in theory statistically sound or even recommended, simulation analyses revealed that it performs poorly; because both the number of results per article and the probability of a gross inconsistency were too low, it was accompanied by a too low Type I error, a lack of statistical power, and inaccurate effect size estimates [@Nuijten2017]. Therefore, following @Wicherts and @Nuijten2016, we tested our hypotheses on statistical reporting errors (H1 and H2) using standard logistic regressions.\     
&nbsp;&nbsp;&nbsp;&nbsp;We also conducted logistic regressions to test our hypotheses on publication bias (H3a, H3b) with exactly reported *p*-values from ‘AllP’ as the dependent variable. Since statcheck interprets results with *p* = .05 as being statistically significant [@statcheck122], *p* = .05 was included in the interval of just significant *p*-values for the logistic regressions. To test our hypothesis on *p*-values reported as marginally significant (H4), we conducted logistic regressions with exactly reported *p*-values in the interval (.05-.10] from ‘AllP’ as the dependent variable. All logistic regression analyses contained a binary predictor indicating whether a result was related to an explicitly stated hypothesis or not. We chose not to include other potentially relevant control variables, such as journal and year of publication, because some analyses had too little data for including multiple predictors.

\vspace{2cm}

# Results
In this section, we start by presenting our results regarding statistical reporting guidelines. Next, results on statistical reporting errors, publication bias, the ‘bump’ in *p*-values, and marginal significance are discussed. For each results-related topic, we first present automatically retrieved descriptive results and (if applicable) results of hypothesis testing. Then, we discuss descriptive statistics of results related to explicitly stated hypotheses from 'Hyp'. Results on specific years and journals that were of little theoretical interest or were based on too little data are not discussed in the text but can be found in the corresponding tables.
\

## Statistical reporting guidelines
Of the 143 sociology journals in 'SRG', one journal (*Society*) did not seem to have any guidelines authors are explicitly required or allowed to follow when preparing their manuscripts. Four journals (2.8%) explicitly required authors to follow guidelines established by the journal itself, and 102 (71.3%) required authors to adhere to (reference) guidelines established by external organizations. Only 13 journals (9.1%) requested authors to adhere to the APA manual, and thereby, to the APA statistical reporting guidelines. See Table 5 for an overview of statistics on sociology journals requesting/allowing adherence to different statistical reporting guidelines.
\pagebreak

```{r Table 5 statistical reporting guidelines, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
data_table5 <- c("", "10 (7%)", "10 (7%)",
                 "", "12 (8.4%)", "3 (2.1%)",
                 "", "7 (4.9%)", "6 (4.2%)",
                 "", "2 (1.4%)", "9 (6.3%)",
                 "1 (0.7%)", "1 (0.7%)", 
                 "34 (23.8%)", "4 (2.8%)", 
                 "37 (25.9%)", "3 (2.1%)", "2 (1.4%)",
                  "1 (0.7%)", "1 (0.7%)", "143 (100%)")
#Putting content table cells into a matrix.
table5 <- matrix(data_table5, ncol = 1, nrow = 22, byrow = TRUE)

#Giving the matrix row names and column names.
rownames(table5) <- c("APA", "Full manual", "Only references", 
                            "ASA", "Full manual", "Only references",  
                            "Chicago", "Full manual", "Only references",  
                            "Harvard", "Full manual", "Only references",   
                            "Oxford", "Style Manual for Authors, Editors and Printers", 
                            
                            "Other", "Own", 
                            "Multiple required", "Multiple required (one is full APA manual)", "Multiple options (one must be chosen)",
                      "Multiple allowed", "Unknown*", "Total")
colnames(table5) <- paste(c("Number of journals (% of total)"))

#Creating Table 5.
table5 %>%
  #format latex, centered alignment, caption,
  kable(booktabs = T, align = "c", caption = "Numbers and percentages of sociology journals in ‘SRG' requesting/allowing adherence to different types of statistical reporting guidelines.") %>% 
  #grouping data
  pack_rows("Required", 1, 19) %>%
  pack_rows("Other, namely...", 20, 22) %>%
  #adding indentation for different groups
  add_indent(c(2:3, 5:6, 8:9, 11:12)) %>%
  #table will appear in correspondence with its place in this document, table is scaled-down to fit page size, font size is 10
  kable_styling(latex_options = c("HOLD_position"), full_width = F) %>%
  kable_styling(font_size = 10) %>% 
  #Adding footnote
  footnote(symbol = "We were unable to find which guidelines authors publishing in the journal \\\\emph{Society} are required or allowed to use. The link on the journal's website that should have provided access to this information gave a ‘page not found’ error.", escape = FALSE, threeparttable = T, footnote_as_chunk = T)
```

## Statistical reporting errors
Of the 505 ‘APA’ results, 69 (13.7%) were inconsistent and 8 (1.6%) grossly inconsistent (see Table 6). All grossly inconsistent results had a statistically significant reported *p*-value and a non-significant recalculated *p*-value. Out of 168 results related to explicitly stated hypotheses, 22 (13.1%) were inconsistent and 4 (2.4%) grossly inconsistent. Out of 337 results not related to explicitly stated hypotheses, 47 (13.9%) were inconsistent and 4 (1.2%) grossly inconsistent. Of the recalculated *p*-values from 'APA', 416 (82.4%) were retrieved from the two APA journals. These journals, *JMF* and *CHQ*, had comparable percentages of inconsistencies (14.6% and 14.7%, respectively) and gross inconsistencies (1.6% and 1.7%, respectively). We found a lower prevalence of inconsistencies among automatically retrieved results for non-APA journals *ASR* and *AJS* (2.3% and 4.9%, respectively). Our hypotheses that less (gross) inconsistencies would be observed for results of explicitly stated hypotheses were not confirmed. As for H1, the odds of a result of an explicitly stated hypothesis being inconsistent were $\frac{1}{.930} \approx 1.076$ times smaller than the odds that a result not related to an explicitly stated hypothesis was inconsistent, *b* = -.073, *p* = .793, OR = .930, 95% CI [.531, 1.585]. Regarding H2, the odds of a result of an explicitly stated hypothesis being grossly inconsistent were 2.030 two times larger than the odds that a result not related to an explicitly stated hypothesis was grossly inconsistent, but this difference was not statistically significant, *b* = .708, *p* = .321, OR = 2.030, 95% CI [.475, 8.685]. Full results of the logistic regressions used to test H1 and H2 can be found in Table S1. Among recalculated *p*-values from ‘Hyp’, 14 out of 404 were inconsistent (3.5%), and 2 (0.5%) were grossly inconsistent. Again, the grossly inconsistent results had a statistically significant reported *p*-value and a nonsignificant recalculated *p*-value. For a comprehensive overview of statistical reporting errors at the article and results level, see Table S2.


```{r Table 6 statistical reporting errors, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
data_table6 <- c("", "", "", "", 
                 "", 168, "22 (13.1%)", "4 (2.4%)", #begin APA
                 "", 337, "47 (13.9%)", "4 (1.2%)", 
                 "", "", "", "", 
                 7, 43, "1 (2.3%)", "1 (2.3%)",
                 3, 41, "2 (4.9%)", "0 (0%)",
                 2, 5, "5 (100%)", "0 (0%)", 
                 36, 185, "27 (14.6%)", "3 (1.6%)",
                 28, 231, "34 (14.7%)", "4 (1.7%)",
                 "", "", "", "", 
                 20, 172, "22 (12.8%)", "1 (0.6%)", 
                 21, 136, "15 (11%)", "3 (2.2%)",
                 35, 197, "32 (16.2%)", "4 (2%)", 
                 76, 505, "69 (13.7%)", "8 (1.6%)", #end APA
                 "", "", "", "",                    #begin Hyp
                 10, 331, "11 (3.3%)", "1 (0.3%)",    #ASR
                 7, 68, "1 (1.5%)", "1 (1.5%)",       #AJS
                 2, 5, "2 (40.0%)", "0 (0%)",         #SQ
                 "", "", "", "", 
                 11, 312, "11 (3.5%)", "1 (0.3%)",    #2014
                 3, 33, "0 (0%)", "0 (0%)",           #2015
                 5, 59, "3 (5.1%)", "1 (1.7%)",       #2016
                 19, 404, "14 (3.5%)", "2 (0.5%)") #end Hyp
#Putting content table cells into a matrix.
table6 <- matrix(data_table6, ncol = 4, nrow = 23, byrow = TRUE)

#Giving matrix row and column names.
rownames(table6) <- c("Relation to hypothesis", "Yes", "No", "Journal", 
                            "ASR", "AJS", "SQ", "JMF", "CHQ", "Year", 
                            "2014", "2015", "2016", "Total", "Journal", 
                            "ASR", "AJS", "SQ", "Year", 
                            "2014", "2015", "2016", "Total")
colnames(table6) <- paste(c("Articles", "Results", "Inconsistencies", "Gross inconsistencies"))

#Creating Table 6.
table6 %>%
  #booktabs used, centered alignment, table continues on next page, caption
  kable(booktabs = T, align = "c", longtable = T, caption = "Descriptive statistics on (gross) inconsistencies for ‘APA' and ‘Hyp'.") %>% 
  #adding indentation for the categories of all variables
  add_indent(c(2,3, 5:9, 11:13, 16:18, 20:22)) %>% 
  #grouping data for the two data sets
  pack_rows("‘APA'", 1, 14) %>%
  pack_rows("‘Hyp'", 15, 23) %>%
  #table will appear in correspondence with its place in this document, table continues on next page
  kable_styling(latex_options = c("striped", "HOLD_position", "repeat_header"), full_width = F) 
```

## Publication bias	
In ‘AllP’, there was no evidence of publication bias. Overall, for binwidth .01, 32 out of 73 results were just significant (43.8%), and for binwidth .02, 64 out of 127 results were just significant (50.4%) (see Table 7 and Figures 2.1A and 2.1B). Splitting data among results not related to hypotheses (Figures 2.2A and 2.2B) and results that were related to hypotheses (Figures 2.3A and 2.3B), percentages were also around 50. Hence, we could not reject H~0~ for our hypotheses on publication bias (H3a and H3b). For H3a, there were $\frac{1}{.877} \approx 1.140$ times less just significant *p*-values among results of explicitly stated hypotheses than among results not related to explicitly stated hypotheses for binwidth .01, *b* = -.132, *p* = .794, OR = .877, 95% CI [.321, 2.345]. In case of H3b for binwidth .02, H~0~ could not be rejected either, since *b* = -.251, *p* = .521, OR = .778, 95% CI [.358, 1.674] (full results of the logistic regressions used to test H3a and H3b can be found in Table S3). Similarly, for ‘Hyp’, no evidence of publication bias was found among results related to explicitly stated hypotheses; we found (slightly) more just significant *p*-values than just nonsignificant ones, namely 9 out of 14 results (64.3%) for binwidth .01, and 14 out of 26 (53.8%) for binwidth .02 (see Figure 3 and Table 7). 

```{r plots publication bias/bump in p-values AllP, warning=FALSE, message = FALSE, echo = FALSE, fig.height=8, fig.cap="Histograms with binwidths .01 and .02 of exactly reported p-values in the interval [.01-.11] from ‘AllP’. Information is provided for the totals of exactly reported p-values, and for exactly reported p-values (not) related to explicitly stated hypotheses."}
library('readxl') #reading in data files
library('ggplot2') #plotting
library('ggtext') #adapting title size
library('gridExtra') #arranging tables
library('gridtext') #adapting text above final table
library('grid') #font (size) options in grid.arrange function
library('ggpubr') #creating one ggplot object out of multiple ggplots
library("cowplot") #adding a label to the final plot

setwd("..") #setting working directory to 'thesis'
setwd("./Data") #setting working directory to 'Data'

#Reading in 'AllP'
AllP <- read_excel("./AllP/AllP.xlsx", col_types = c(
#Article_numb, Article, Author(s), Journal
"numeric", "text", "text", "text", 
#Year, Statistic, Reported.Comparison, Reported.P.Value
"numeric", "text", "text", "numeric",
#Raw, Result.Table, Reported.Not.Relevant, Not.Reported
"text", "numeric", "numeric", "numeric",
#Model.fit, Result.Hypothesis, Marg.Sig, Phrase in which mentioned
"numeric", "numeric", "numeric", "text", 
#explicitly stated hypothesis, Comments
"text", "text"))

#Selecting only exactly reported p-values for which the relation to an explicitly stated hypothesis is clear.
AllP_exact <- subset(AllP, Reported.Comparison == "=" & !is.na(AllP$Result.Hypothesis)) 



###################################################
##     Plots total p-values range [.01-.11]      ##  
###################################################

#Plot binwidth .01. 
#sum(AllP_exact$Reported.P.Value >= 0.01 & AllP_exact$Reported.P.Value <= .11) #n = 331
AllP_binw_01 <- ggplot() + 
  geom_histogram(data = AllP_exact, aes(x = Reported.P.Value), fill = 'grey', colour='black', bins=10, binwidth = .01, boundary = 0.01, closed = "right") + #10 bins, width .01, black bin lines, grey bins, lines of bins are black, bins close on the right
  scale_x_continuous(expand = c(0, 0), breaks = seq(.01, .11, by = .01), lim = c(.005,.115)) + #ticks on the x-axis with intervals of .01         
  scale_y_continuous(expand = c(0, 0), breaks = seq(0,200, by = 25)) + #ticks on the y-axis with intervals of 25                               
  labs(title = "2.1A: total, binwidth .01", x = "*p*-value") +  #title and label x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines,  no gridline is chosen
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  theme(plot.title = ggtext::element_markdown(size = 12), axis.title.x = element_markdown()) #enabling markdown text in label x-axis, title size 12

#Plot binwidth .02. 
AllP_binw_02 <- ggplot() + 
  geom_histogram(data = AllP_exact, aes(x = Reported.P.Value), fill = 'grey', colour='black', bins=10, binwidth = .02, boundary = .01, closed = "right") + #10 bins, width .01, black bin lines, grey bins, lines of bins are black, bins close on the right
  scale_x_continuous(expand = c(0, 0), breaks = seq(.01, .11, by = .02), lim = c(0,.115)) + #ticks on the x-axis       
  scale_y_continuous(expand = c(0, 0), breaks = seq(0,125, by = 25), lim = c(0, 125)) + #ticks on the y-axis                           
  labs(title = "2.1B: total, binwidth .02", x = "*p*-value") +  #title and label x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines,  no gridline is chosen
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  theme(plot.title = ggtext::element_markdown(size = 12), axis.title.x = element_markdown()) #enabling markdown text in label x-axis, title size 12



###################################################
##     Plots results not related to hypothesis   ##  
###################################################

#Creating new data sets containing only reported results related to explicitly stated hypotheses and results not related to explicitly stated hypotheses, respectively.
AllP_exact_nh <- subset(AllP_exact, Result.Hypothesis == 0) #data set containing only results of explicitly stated hypotheses
AllP_exact_h <- subset(AllP_exact, Result.Hypothesis == 1)  #data set containing only results not related to explicitly stated hypotheses

#Plot binwidth .01.
#sum(AllP_exact_nh$Reported.P.Value >= 0.01 & AllP_exact_nh$Reported.P.Value <= .11) #n = 234
AllP_binw_01_nh <- ggplot() + 
  geom_histogram(data = AllP_exact_nh, aes(x = Reported.P.Value), fill = 'grey', colour='black', bins=10, binwidth = .01, boundary = 0.01, closed = "right") + #10 bins, width .01, black bin lines, grey bins, bins close on the left
  scale_x_continuous(expand = c(0, 0), breaks = seq(.01, .11, by = .01), lim = c(.005,.115)) + #ticks on the x-axis     
  scale_y_continuous(expand = c(0, 0), breaks = seq(0,200, by = 25)) + #ticks on the y-axis                                 
  labs(title = "2.2A: no hypothesis, binwidth .01", x = "*p*-value") +  #title and label x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines, no gridline
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  theme(plot.title = ggtext::element_markdown(size = 12), axis.title.x = element_markdown()) #enabling markdown text in label x-axis, title size 12

#Plot binwidth .02.
AllP_binw_02_nh <- ggplot() + 
  geom_histogram(data = AllP_exact_nh, aes(x = Reported.P.Value), fill = 'grey', colour='black', bins=10, binwidth = .02, boundary = .01, closed = "right") + #10 bins, width .01, black bin lines, grey bins, bins close on the left
  scale_x_continuous(expand = c(0, 0), breaks = seq(.01, .11, by = .02), lim = c(0,.115)) + #ticks on the x-axis   
  scale_y_continuous(expand = c(0, 0), breaks = seq(0,125, by = 25), lim = c(0, 125)) + #ticks on the y-axis                          
  labs(title = "2.2B: no hypothesis, binwidth .02", x = "*p*-value") +  #title and label x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines, no gridline
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  theme(plot.title = ggtext::element_markdown(size = 12), axis.title.x = element_markdown()) #enabling markdown text in label x-axis, title size 12



##################################################
##     Plots results related to hypothesis      ##  
##################################################

#Plot binwidth .01.
AllP_binw_01_h <- ggplot() + 
  geom_histogram(data = AllP_exact_h, aes(x = Reported.P.Value), fill = 'grey', colour='black', bins=10, binwidth = .01, boundary = .01,, closed = "right") + #10 bins, width .01, black bin lines, grey bins, bins close on the left
  scale_x_continuous(expand = c(0, 0), breaks = seq(.01, .11, by = .01), lim = c(.005,.115)) +     
    scale_y_continuous(expand = c(0, 0), breaks = seq(0,25, by = 5)) + #ticks on the x-axis and y-axis                         
  labs(title = "2.3A: result hypothesis, binwidth .01", x = "*p*-value") +  #title and label x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines, no gridline
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  theme(plot.title = ggtext::element_markdown(size = 12), axis.title.x = element_markdown()) #enabling markdown text in label x-axis, title size 12

#Plot binwidth .02.
AllP_binw_02_h <- ggplot() + 
  geom_histogram(data = AllP_exact_h, aes(x = Reported.P.Value), fill = 'grey', colour='black', bins=10, binwidth = .02, boundary = .01, closed = "right") + #10 bins, width .01, black bin lines, grey bins, bins close on the left
  scale_x_continuous(expand = c(0, 0), breaks = seq(.01, .11, by = .02), lim = c(0,.115)) +     
    scale_y_continuous(expand = c(0, 0), breaks = seq(0, 40, by = 10), lim = c(0, 40)) + #ticks on the x-axis and y-axis                                 
  labs(title = "2.3B: result hypothesis, binwidth .02", x = "*p*-value") +  #title and label x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines, no gridline
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  theme(plot.title = ggtext::element_markdown(size = 12), axis.title.x = element_markdown()) #enabling markdown text in label x-axis, title size 12



###########################
##     Final figure      ##  
###########################

#Putting the plots created above in one figure. 
notes_plots_AllP <- c("*Note*. For 2.1A & 2.1B, *n* = 331. For 2.2A & 2.2B, *n* = 234. For 2.3A & 2.3B, *n* = 97.") #notes of the figure
plots_AllP <- grid.arrange(AllP_binw_01, AllP_binw_02, AllP_binw_01_nh, AllP_binw_02_nh, AllP_binw_01_h, AllP_binw_02_h, ncol=2, bottom = richtext_grob(notes_plots_AllP, hjust = .571, gp = gpar(fontfamily = "serif"))) #creating one figure with four plots, and adding a note (left-justified, serif font)
plots_AllP <- as_ggplot(plots_AllP) #putting created figure into ggplot format
```


```{r plots publication bias/bump in p-values Hyp, warning=FALSE, message = FALSE, echo = FALSE, fig.height=3, fig.cap="Histograms with binwidths .01 (3A) and .02 (3B) of exactly reported p-values related to explicitly stated hypotheses in the interval [.01-.11] from 'Hyp'."}
setwd("..") #setting working directory to 'thesis'
setwd("./Data") #setting working directory to 'Data'

Hyp <- read_excel("./Hyp & inclusion_Hyp/Hyp.xlsx", col_types = c(
#Article_numb, Article, Author(s), Journal
"numeric", "text", "text", "text", 
#Year, Issue, Number of hypotheses, Belongs to hypothesis
"numeric", "numeric", "numeric", "text", 
#Statistic, b, se, r, t_stat
"text", "numeric", "numeric", "numeric", "numeric",
#n, IVs, df_1, df_2
"numeric", "numeric", "numeric", "numeric", 
#Reported.Comparison, Reported.P.Value, lb_b, ub_b 
"text", "numeric", "numeric", "numeric",
#lb_se, ub_se, lb_r, ub_r, lb_stat, ub_stat,
"numeric", "numeric", "numeric", "numeric", "numeric", "numeric",
#dec_pval_rep, Computed, P_one_sided_chi, Error
"numeric", "numeric", "numeric", "numeric", 
#DecisionError, OneTailed, Marg.sig, Hypothesis
"numeric", "text", "numeric", "text", 
#Information from text article used, Comment
"text", "text"))

#Making '1E-3' and 'ns' numeric, such that 'Reported.P.Value' can be converted to a numeric variable properly.
Hyp$Reported.P.Value<- as.numeric(Hyp$Reported.P.Value)
#Subset exactly reported p-values in range [.0-.10].
Hyp_all_jrnl_exact <- subset(Hyp, Reported.Comparison == "=")
pb_bump_Hyp <- subset(Hyp_all_jrnl_exact, Reported.P.Value >= 0.01 & Reported.P.Value <= .11) 
#nrow(pb_bump_Hyp) #n = 67
#Keeping only the variable 'Reported.P.Value' for plotting purposes.
pb_bump_Hyp <- pb_bump_Hyp[ ,c("Reported.P.Value")]

#Plot binwidth .01.
Hyp_binw_01 <- ggplot() + 
    geom_histogram(data = pb_bump_Hyp, aes(x = Reported.P.Value), fill = 'grey', colour='black', #10 bins of width .01, black bin lines, grey bins, bins close on the left
                 bins=10, binwidth = .01,  boundary = .01, closed = "right") + #ticks on the x-axis with intervals of .01
  scale_x_continuous(expand = c(0, 0), breaks = seq(.01, .11, by = .01), lim = c(.005,.115)) + #ticks on the x-axis in the range [0, .105] with intervals of .01
  scale_y_continuous(expand = c(0, 0), breaks = seq(0, 15, by = 5)) + #ticks on the y-axis in the range [0, 200] +
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  labs(title = "3A: binwidth .01", x = "*p*-value") +  #title and label x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines and no gridline 
  theme(plot.title = ggtext::element_markdown(size = 12)) + #adapt title size
  theme(axis.title.x = ggtext::element_markdown()) #enabling markdown text in label x-axis

#Plot binwidth .02.
#sum(Hyp_all_jrnl_exact$Reported.P.Value >= 0.01 & Hyp_all_jrnl_exact$Reported.P.Value <= .11) #n = 67
Hyp_binw_02 <- ggplot() + 
    geom_histogram(data = Hyp_all_jrnl_exact, aes(x = Reported.P.Value), fill = 'grey', colour='black', #10 bins of width .01, black bin lines, grey bins, closing left
                 bins=10, binwidth = .02, boundary = .01, closed = "right") + #ticks x-axis, intervals of .01
  scale_x_continuous(expand = c(0, 0), breaks = seq(.01, .11, by = .02), lim = c(0,.115)) + #ticks on the x-axis in the range [0, .105] with intervals of .01
  scale_y_continuous(expand = c(0, 0), breaks = seq(0, 40, by = 10), lim = c(0, 40)) + #ticks on the y-axis in the range [0, 200] 
  geom_vline(xintercept = .05, linetype = "dotted") + #dotted vertical line at p = .05 
  labs(title = "3B: binwidth .02", x = "*p*-value") +  #title label x-axis
  theme_classic(base_size = 10) + #theme with x and y axis lines and no gridline 
  theme(plot.title = ggtext::element_markdown(size = 12)) + #adapt title size
  theme(axis.title.x = ggtext::element_markdown()) #enabling markdown text in label x-axis

#Putting the plots created above in one figure. 
note_plots_Hyp <- c("*Note*. *n* = 67.") #note plot
plots_Hyp <- grid.arrange(Hyp_binw_01, Hyp_binw_02, ncol=2, nrow=1, bottom = richtext_grob(note_plots_Hyp, hjust = 3.75, gp = gpar(fontfamily = "serif"))) #creating one figure with two plots, and adding a note (left-justified, serif font)
plots_Hyp <- as_ggplot(plots_Hyp) #putting created figure into ggplot format
```


\clearpage
```{r Table 7 publication bias, echo = FALSE, warning = FALSE, message = FALSE}
#Creating content table cells.
data_table7 <- c("", "", "", "",  "", "",
                 "10 (41.7%)", 14, 24, "17 (45.9%)", 20, 37,  #begin AllP, relation to hypothesis
                 "22 (44.9%)", 27, 49, "47 (52.2%)", 43, 90,  #no relation to hypothesis
                 "", "", "", "", "", "",
                 "8 (40%)", 12, 20, "11 (37.9%)", 18, 29,     #ASR
                 "3 (42.9%)", 4, 7, "8 (57.1%)", 6, 14,       #AJS
                 "1 (20%)", 4, 5, "3 (42.9%)", 4, 7,          #SQ   
                 "14 (43.8%)", 18, 32, "28 (49.1%)", 29, 57,  #JMF
                 "6 (66.7%)", 3, 9, "14 (70%)", 6, 20,        #CHQ
                 "", "", "", "", "", "", 
                 "13 (44.8%)", 16, 29, "27 (55.1%)", 22, 49,  #2014
                 "10 (52.6%)", 9, 19, "20 (52.6%)", 18, 38,   #2015
                 "9 (36%)", 16, 25, "17 (42.5%)", 23, 40,     #2016
                 "32 (43.8%)", 41, 73, "64 (50.4%)", 63, 127, #total, end AllP
                 "", "", "", "",  "", "",                     #begin Hyp
                 "2 (40%)", 3, 5, "5 (45.5%)", 6, 11,         #ASR
                 "3 (100%)", 0, 3, "4 (80%)", 1, 5,           #AJS
                 "4 (66.7%)", 2, 6, "5 (50%)", 5, 10,         #SQ
                 "", "", "", "",  "", "",                     
                 "4 (66.7%)", 2, 6, "8 (61.5%)", 5, 13,       #2014
                 "1 (50%)", 1, 2, "1 (50%)", 1, 2,            #2015
                 "4 (66.7%)", 2, 6, "5 (45.5%)", 6, 11,       #2016
                 "9 (64.3%)", 5, 14, "14 (53.8%)", 12, 26)    #total, end Hyp
#Putting content table cells into a matrix.
table7 <- matrix(data_table7, ncol = 6, nrow = 23, byrow = TRUE)

#Giving matrix row names and column names.
rownames(table7) <- paste(c("Relation to hypothesis", "Yes", "No", "Journal", 
                            "ASR", "AJS", "SQ", "JMF", "CHQ", "Year", 
                            "2014", "2015", "2016", "Total", "Journal", 
                            "ASR", "AJS", "SQ", "Year", 
                            "2014", "2015", "2016", "Total"))
colnames(table7) <- paste(c("(.04-.05]", "(.05-.06]", "Total", "(.03-.05]", "(.05-.07]", "Total"))

#Creating Table 7.
table7 <- table7 %>%
  #booktabs used, centered alignment, caption
  kable(booktabs = T, align = "c", caption = "Descriptive statistics on publication bias among reported p-values for ‘AllP’ and ‘Hyp’.") %>% 
  #table will appear in correspondence with its place in this document, table is scaled-down to fit page size
  kable_styling(latex_options = c("striped", "HOLD_position"), full_width = F) %>%
  kable_styling(font_size = 10) 

table7 %>%
  #Adding indentation for the categories of all variables
  add_indent(c(2,3, 5:9, 11:13, 16:18, 20:22)) %>% 
  #grouping columns based on binwidth
  add_header_above(c(" ", "Binwidth .01" = 3, "Binwidth .02" = 3)) %>%
  #grouping rows for the two data sets
  pack_rows("‘AllP'", 1, 14) %>%
  pack_rows("‘Hyp'", 15, 23) %>%
  #fixing large spaces between columns binwidth .02
  column_spec(4:6, width = "5em") 
```

\pagebreak

\hspace{10em}

## Bump in *p*-values
Overall, Table 8, Figure 2, and Figure 3 show no ‘bump’ in *p*-values in ‘AllP’ or ‘Hyp'. Using binwidth .01, higher *p*-value intervals contained 32 out of 64 *p*-values (50%) for ‘AllP’ overall (Figure 2.1A), 22 out of 47 (46.8%) (Figure 2.2A) for results not related to hypotheses from ‘AllP’, 10 out of 17 (58.8%) for results related to hypotheses from ‘AllP’ (Figure 2.3A), and 9 out of 14 (64.3%) for results related to hypotheses from ‘Hyp’ (Figure 3A). Correspondingly, for binwidth .02, the higher *p*-value intervals contained 64 out of 184 (34.8%) (Figure 2.1B), 47 out of 133 (35.3%)(Figure 2.2B), 17 out of 51 (33.3%) (Figure 2.3B), and 14 out of 37 (37.8%) results (Figure 3B), respectively.


\pagebreak

```{r Table 8 bump in p-values, echo = FALSE, warning = FALSE, message = FALSE}
#Creating content table cells.
data_table8 <- c("", "", "", "",  "", "",
                  7, "10 (58.8%)", 17, 34, "17 (33.3%)", 51, #begin AllP
                 25, "22 (46.8%)", 47, 86, "47 (35.3%)", 133,
                 "", "", "", "", "", "",
                 3, "8 (72.7%)", 11, 25, "11 (30.6%)", 36,
                 5, "3 (37.5%)", 8, 23, "8 (25.8%)", 31,
                 2, "1 (33.3%)", 3, 1, "3 (75%)", 4, 
                 14, "14 (50%)", 28, 44, "28 (38.9%)", 72,
                 8,"6 (42.9%)", 14, 27, "14 (34.1%)", 41,
                 "", "", "", "", "", "", 
                 14, "13 (48.1%)", 27, 51, "27 (34.6%)", 78,
                 10, "10 (50%)", 20, 41,"20 (32.8%)", 61,
                 8, "9 (52.9%)", 17, 28, "17 (37.8%)", 45,
                 32, "32 (50%)", 64, 120, "64 (34.8%)", 184, #end AllP
                 "", "", "", "",  "", "",                    #begin Hyp
                 3, "2 (40%)", 5, 9, "5 (35.7%)", 14,
                 1, "3 (75%)", 4, 11, "4 (26.7%)", 15,
                 1, "4 (80%)", 5, 3, "5 (62.5%)", 8,        
                 "", "", "", "",  "", "",                   
                 4, "4 (50%)", 8, 14, "8 (36.4%)", 22,
                 0, "1 (100%)", 1, 7, "1 (12.5%)", 8,
                 1, "4 (80%)", 5, 2, "5 (71.4%)", 7,
                 5, "9 (64.3%)", 14, 23, "14 (37.8%)", 37) #end Hyp
#Putting content table cells into a matrix.
table8 <- matrix(data_table8, ncol = 6, nrow = 23, byrow = TRUE)

#Giving matrix row and column names.
rownames(table8) <- paste(c("Relation to hypothesis", "Yes", "No", "Journal", 
                            "ASR", "AJS", "SQ", "JMF", "CHQ", "Year", 
                            "2014", "2015", "2016", "Total", "Journal", 
                            "ASR", "AJS", "SQ", "Year", 
                            "2014", "2015", "2016", "Total"))

#Creating Table 8.
table8 <- table8 %>%
  #booktabs used, centered alignment, caption
  kable(booktabs = T, align = "c", 
       col.names = c("(.03-.04]", "(.04-.05]", "Total", "(.01-.03]", "(.03-.05]", "Total"),
       caption = "Descriptive statistics on the bump in just significant reported p-values for ‘AllP' and ‘Hyp'.") %>% 
  #table will appear in correspondence with its place in this document, table is scaled-down to fit page size
  kable_styling(latex_options = c("HOLD_position", "scale_down"), full_width = F) 

table8 %>%
  #adding indentation for the categories of all variables
  add_indent(c(2,3, 5:9, 11:13, 16:18, 20:22)) %>% 
  #grouping columns based on binwidth
  add_header_above(c(" ", "Binwidth .01" = 3, "Binwidth .02" = 3)) %>% 
  #grouping variables for the two data sets
  pack_rows("‘AllP'", 1, 14) %>%
  pack_rows("‘Hyp'", 15, 23) %>%
  #fixing large spaces between columns binwidth .02
  column_spec(4:6, width = "5em")  
```
\pagebreak

## Marginal significance
In 46 out of 107 articles from 'AllP' (43%) with reported *p*-values in the interval (.05-.10], at least one *p*-value in this interval was reported as marginally significant (see Table 9). Out of 206 'AllP' results with reported *p*-values in the interval (.05-.10], 72 (35%) were reported as marginally significant. For results not related to hypotheses and results related to hypotheses, this was the case for 52 out of 136 (38.2%) and 20 out of 70 (28.6%) *p*-values in the interval (.05-.10], respectively. Among journals, the prevalence of *p*-values to which marginal significance was assigned was highest in *CHQ* (11 out of 21 results, or 52.4%) and lowest in *AJS* (4 out of 31 results, or 12.9%). The prevalence of assignment of marginal significance to *p*-values was comparable between years (33.8%-35.7%). Our hypothesis that assignment of marginal significance is less prevalent among results related to explicitly stated hypotheses in 'AllP' (H4) is not confirmed, *b* = -.437, *p* = .170, OR = .646, 95% CI [.342, 1.194]. Full results of the logistic regression used to test H4 can be found in Table S4.\
&nbsp;&nbsp;&nbsp;&nbsp;Table 9 also shows the prevalence of marginal significance in ‘Hyp’. In 19 of the 30 articles with *p*-values in the interval (.05-.10] from ‘Hyp’ (63.3%), at least one *p*-value in this interval was reported as marginally significant. At the results level, Table 9 shows that overall, 106 of 130 (81.5%) *p*-values in the interval (.05-.10] were reported as marginally significant. Reporting results as marginally significant was most prevalent in *AJS* (72 out of 76 results, or 94.7%) and least prevalent in *SQ* (5 out of 12 results, or 41.7%). Among the different years, reporting results as marginally significant was most prevalent in 2015 (71 out of 74 results, or 95.9%) and least prevalent in 2014 (11 out of 24 results, or 45.8%).\
&nbsp;&nbsp;&nbsp;&nbsp;Interestingly, the percentage of *p*-values in interval (.05-.10] reported as marginally significant was much higher for manually retrieved statistical results related to explicitly stated hypotheses in ‘Hyp’ (81.5%)  than for reported *p*-values in ‘AllP’ (35%) (*z* = 8.33, *p* < .001, *z*-test for two independent proportions). 

```{r Table 9 marginal significance, echo = FALSE, warning = FALSE, message = FALSE}
#Creating content table cells.
data_table9 <- c("", "", "", "",  "", "",
                 "", "", "", "20 (28.6%)", 50, 70,           #begin AllP
                 "", "", "", "52 (38.2%)", 84, 136,
                 "", "", "", "", "", "",
                 "11 (44%)", 14, 25, "19 (32.8%)", 39, 58, 
                 "3 (20%)", 12, 15, "4 (12.9%)", 27, 31,
                 "3 (37.5%)", 5, 8, "4 (36.4%)", 7, 11, 
                 "23 (47.9%)", 25, 48, "34 (40%)", 51, 85,   
                 "6 (54.5%)", 5, 11, "11 (52.4%)", 10, 21,
                 "", "", "", "", "", "", 
                 "11 (35.5%)", 20, 31, "19 (35.2%)", 35, 54, 
                 "17 (44.7%)", 21, 38,"30 (35.7%)", 54, 84,
                 "18 (47.4%)", 20, 38, "23 (33.8%)", 45, 68,
                 "46 (43%)", 61, 107, "72 (35%)", 134, 206,  #end AllP
                 "", "", "", "",  "", "",                    #begin Hyp
                 "8 (72.7%)", 3, 11, "29 (69%)", 13, 42,      #ASR
                 "9 (60%)", 6, 15, "72 (94.7%)", 4, 76,       #AJS
                 "2 (50%)", 2, 4, "5 (41.7%)", 7, 12,         #SQ
                 "", "", "", "",  "", "",                   
                 "6 (46.2%)", 7, 13, "11 (45.8%)", 13, 24,    #2014
                 "6 (85.7%)", 1, 7, "71 (95.9%)", 3, 74,      #2015
                 "7 (70%)", 3, 10, "24 (75%)", 8, 32,         #2016
                 "19 (63.3%)", 11, 30, "106 (81.5%)", 24, 130)  #end Hyp
#Putting content table cells into a matrix.
table9 <- matrix(data_table9, ncol = 6, nrow = 23, byrow = TRUE)

#Giving matrix row names and column names.
rownames(table9) <- paste(c("Relation to hypothesis", "Yes", "No", "Journal",
                            "ASR", "AJS", "SQ", "JMF", "CHQ", "Year", 
                            "2014", "2015", "2016", "Total", "Journal", 
                            "ASR", "AJS", "SQ", "Year", 
                            "2014", "2015", "2016", "Total"))
colnames(table9) <- paste(c("Yes", "No", "Total", "Yes", "No", "Total"))

#Creating Table 9.
table9 <- table9 %>%
  #booktabs used, centered alignment, caption, table continues on next page
  kable(booktabs = T, align = "c", longtable = T, caption = "Descriptive statistics on marginal significance among reported p-values for ‘AllP’ and ‘Hyp’.") %>% 
  #table will appear in correspondence with its place in this document, continues on next page
  kable_styling(latex_options = c("HOLD_position", "repeat_header"), full_width = F) 

table9 %>%
  #adding indentation for the categories of all variables
  add_indent(c(2,3, 5:9, 11:13, 16:18, 20:22)) %>% 
  #grouping columns based on binwidth
  add_header_above(c(" ", "Article level" = 3, "Results level" = 3)) %>% 
  #grouping variables for the two data sets
  pack_rows("‘AllP'", 1, 14) %>%
  pack_rows("‘Hyp'", 15, 23) #%>%
```
\vspace{2cm}

# Discussion
In this article, we studied different aspects of statistical reporting in sociology. Statistical reporting is of high quality if results are reproducible and do not contain errors, and if clear communication and critical evaluation of results within a discipline is possible due to standardized reporting. We created a data set on reporting guidelines in sociology with which we studied statistical reporting guidelines of 143 sociology journals. We also created three data sets to study statistical results reported in sociology articles. These data sets contained either results retrieved automatically by statcheck from the 2014-2016 volumes of *ASR*, *AJS*, *SQ*, *JMF*, and *CHQ*, or manually retrieved results related to explicitly stated hypotheses from the 2014-2016 volumes of *ASR*, *AJS*, and *SQ*. More specifically, we studied statistical reporting errors among 505 automatically retrieved APA-reported results and 404 manually retrieved results related to explicitly stated hypotheses. Furthermore, we studied publication bias, the ‘bump’ in *p*-values, and marginal significance among 2,960 automatically retrieved *p*-values and 4,929 manually retrieved *p*-values related to explicitly stated hypotheses.\   
&nbsp;&nbsp;&nbsp;&nbsp;One important result of our study is that for all our hypotheses, no differences between results related to explicitly stated hypotheses and other results were found. Possibly, sociology authors are not inclined to report more biased statistical results for explicitly stated hypotheses. Furthermore, they seemingly do not see the need to 'boost' the evidential value of *p*-values related to hypotheses in the interval (.05-.10] to create an (unwarranted) impression of a true effect. It should be noted that, in hindsight, we do not see why inconsistencies that do not change a reported result's statistical significance would occur more often among results related to hypotheses than among other results (H1). Instead, a more plausible hypothesis would be that authors pay more attention to the accuracy of their most important results. Alternatively, authors could be more precise in writing down statistically significant results of hypotheses than they are in writing down non-significant results of hypotheses, since the former might be more interesting to them due to publication pressure. Finally, an explanation for not finding differences between results related to explicitly stated hypotheses and other results is that there is an actual true effect but that statistical power of our tests was analyses lacking (505 observations, our largest sample size used for logistic regressions, is not sufficient to detect small or even medium true effect sizes). However, our sample statistics also did not hint at substantial effects of results (not) being related to explicitly stated hypotheses. \   
&nbsp;&nbsp;&nbsp;&nbsp;We found a lack of requested adherence to statistical reporting guidelines within sociology journals: only 9.1% required adherence to the APA statistical reporting guidelines. This is especially troublesome because none of the other guidelines authors were allowed or required to follow by sociology journals had statistical reporting guidelines that ensure reproducibility of results. For this reason, a lack of adherence to the APA statistical reporting guidelines may hamper reproducibility of results, which in turn may negatively influence statistical reporting quality in sociology. Reproducible results can namely be easily assessed by third parties and may thereby motivate authors to thoroughly check if their results are reported correctly. Hence, reproducibility of results could reduce the prevalence of statistical reporting errors. Concretely, for statistical reporting guidelines to ensure reproducible reporting, they should require that authors explicitly state for each analysis or result the test statistic, *df* (if applicable), *p*-value, and whether the test was one- or two-tailed. All these requirements are covered by the APA statistical reporting guidelines (see APA, 2022a).\    
&nbsp;&nbsp;&nbsp;&nbsp;We found that among automatically retrieved statistical APA-reported results from sociology articles, 13.7% were inconsistent and 1.6% grossly inconsistent. Previous research in psychology found a slightly lower but comparable prevalence of inconsistencies of 4.3%-12.8% [@Bakker2011; @Krawczyk; @Nuijten2016; @Veldkamp; @Vermeulen; @Wicherts]. The prevalence of gross inconsistencies was comparable to the 0.8%-2.5% previously found in psychology [@Bakker2011; @Nuijten2016; @Veldkamp; @Vermeulen]. Moreover, all gross inconsistencies we found had a significant reported *p*-value and a nonsignificant recalculated *p*-value. The phenomenon of there being less significant recalculated *p*-values than reported ones is in line with research by @Vermeulen and @Nuijten2016 in psychology. Although these results suggest that the prevalence and direction of statistical reporting errors is similar in both fields, we do not know whether these phenomena generalize to the 11 other APA-journals of sociology. Finally, and interestingly, we found lower prevalences of inconsistencies among automatically retrieved results for non-APA journals *ASR* and *AJS*, suggesting that the absence of APA statistical reporting guidelines does not seem to negatively impact the prevalence of statistical reporting errors in sociology. However, we should be careful interpreting this finding, since these two non-APA-journals are not representative but are considered top journals in the field.  In any case, as has been suggested for psychology [see @Nuijten2016; @Nuijten2017], sociology would profit from using statcheck in the review or editorial phase to prevent statistical reporting errors in the published literature.
\   
&nbsp;&nbsp;&nbsp;&nbsp;Overall, we found no evidence of publication bias in sociology. This contrasts with @Gerber2008, who did find evidence of publication bias among results related to explicitly stated hypotheses in *ASR*, *AJS*, and *SQ*. Of course, it could be that publication bias was simply not present in the articles we studied, but difference in methodology may also be at least partially responsible for the different findings. @Gerber2008 retrieved substantially more results because they used reported *z*-values and *t*-values converted to *z*-values, and they calculated *z*-values using regression coefficients and standard errors. Because mixing recalculated and reported *p*-values is not recommended in analyses of reporting practices [see @Hartgerink2016], we decided to study only reported *p*-values, resulting in relatively little data and hence low power to detect publication bias (if present). However, it is not clear why this different methodology would affect results of analyses on publication bias. As both direct and indirect evidence of publication bias in the related field of psychology is strong [see @Lakens2015b reevaluation of Masicampo & Lalande, 2012; @Kuhberger], and Gerber and Malhotra (2008) also found publication bias in sociology, we recommend more research in sociology, both on the process from conception to submission and acceptance of articles - as has previously been done in psychology by, e.g., @Coursol and @Franco2014 - and using more data on reported statistical results.\   
&nbsp;&nbsp;&nbsp;&nbsp;We also did not find evidence of a ‘bump’ in *p*-value distributions of statistical results in sociology. This is in line with our own expectations and with @Hartgerink2016, who did not find consistent indications of a 'bump' in several psychology journals, and in accordance with the reanalysis of @Masicampo by @Lakens2015b. Following @Hartgerink2016, we conclude that the absence of a 'bump' does not say anything, whereas a 'bump' suggests the (extensive) use of *p*-hacking in a field, probably accompanied by zero or small true effect sizes.\   
&nbsp;&nbsp;&nbsp;&nbsp;Assignment of marginal significance to *p*-values in the interval (.05-.10] in articles was rather common. Among automatically retrieved results, its prevalence of 35% at the results level was somewhat lower than the almost 40% found by @OhlssonCollentine in psychology. This suggests there is no large systemic difference in the reporting of marginal significance in both fields. Noteworthy is the much higher prevalence of marginal significance among manually retrieved results related to hypotheses (81.5%). We mainly attribute this difference to authors being aware that *p*-values in the interval (.05-.10] are of low evidential value, which prevents them from assigning marginal significance to these results in the articles' text [@OhlssonCollentine], whereas assigning marginal significance to these results in tables and figures may seem less harmful to them. The number of articles with at least one *p*-value in the interval (.05-.10] to which marginal significance was assigned was also higher among manually retrieved results related to hypotheses (63.3%) than among automatically retrieved results (43%). Again, this may due to authors being more wary of assigning marginal significance to in-text *p*-values in the interval (.05-.10]. Generally, in line with previous findings in psychology [@Pritschet; @OhlssonCollentine], it can be concluded that sociology authors commonly assign marginal significance to *p*-values in the interval (.05-.10]. \    
&nbsp;&nbsp;&nbsp;&nbsp;To conclude, we advise the sociological field to adopt the APA statistical reporting guidelines or establish its own set of statistical reporting guidelines. Besides improving reproducibility and the information value of results, this would enhance standardization, and thus, comparability of statistical results in sociology, and prevent reporting inconsistent statistical results by implementing the use of statistical checking programs such as statcheck. Enforcing statistical reporting guidelines would also be helpful for meta-research on statistical reporting, publication bias, and research practices. However, for statistical reporting guidelines to be most effective, it would be very helpful if journals and the ASA would actively enforce them. Otherwise, as @Krawczyk has shown for inexactly reported *p*-values in experimental psychology APA-journals, suboptimal research practices may still seep through.
\
\

## Limitations & suggestions for future research 
Our study has some limitations related to amounts of retrieved data. Firstly, we retrieved relatively little data on publication bias and the bump in *p*-values, rendering us unable to provide firm conclusions on the presence of these phenomena in sociology. This is likely due to most *p*-values in sociology being reported inexactly and many results not being reported in-text (also compared to psychology), but in tables and figures instead. In hindsight, we should have included more articles to obtain enough data to properly study publication bias and the 'bump' in *p*-values.\      
&nbsp;&nbsp;&nbsp;&nbsp;Methodologically, there were disadvantages to automatically retrieving data in our study. As mentioned before, most *p*-values in sociology articles are reported in tables and can thus only be retrieved manually. Furthermore, many results are reported in text, but are not APA-reported, rendering statcheck unable to retrieve them. This was especially the case for the non-APA journals in our study. Given the tendency to report *p*-values and statistical results in tables, it is quite likely that the in-text reported *p*-values and APA-reported results retrieved by statcheck are a selective set of all reported *p*-values and APA-reported/reproducible results: they are likely the most theoretically relevant results reported in a study. This has consequences for comparisons of our findings to previous studies in psychology, where it seems to be more common to also report more mundane *p*-values and APA-reported/reproducible results in text: it is likely wise not to make one-to-one translations from the parts of our study using statcheck to studies in psychology that use statcheck.    
&nbsp;&nbsp;&nbsp;&nbsp; Considering the above, it seems that at present, automatic retrieval from the text of articles is methodologically not the best way of collecting data on statistical reporting quality in sociology. Instead, future research on this topic would ideally focus on retrieving statistical data from text, tables, and figures. At present, this can only be done manually, which is a very tedious process. It is therefore vital for reproductive and meta-analytical research purposes that software to extract *p*-values and reproducible results from tables and figures will be developed and/or improved. Only the ability to automatically  extract statistical data from figures and tables will enable high-quality non-tedious assessment of statistical reporting quality in sociology. Steps towards automatically extracting data from figures for meta-analytical purposes have been undertaken in recent years. @Pick, e.g., have developed a promising R package called metaDigitise, which extracts descriptive statistics such as standard deviations, means, correlations, and CIs from several often-used types of plots from articles without major problems. While metaDigitise does not retrieve all types of individual statistics and fully reported results from all types of figures, its applicability can probably be broadened. Automatic data extraction from tables, on the other hand, seems to have its fair share of problems. In a review article of proposals for data extraction from the increasingly popular HTML-tables, @Roldan found that extraction from tables is inhibited by, e.g., the inability to identify multi-part cells, context-data cells, and split headers, and the inability to analyze the structure of a cell’s contents. This leads to data extraction only being possible for certain relatively simple types of tables, which is suboptimal in a meta-analytical context. Thus, substantial work will have to be done to enable automatic extraction of statistical data from tables.  
&nbsp;&nbsp;&nbsp;&nbsp;Finally, it would certainly be useful for future research to track the progress made in sociology in implementing statistical reporting guidelines. If substantial progress is made, it would be interesting to study whether progress seems to have a positive impact on statistical reporting quality in sociology. Then, future research can provide recommendations on how the quality of statistical reporting within sociology could be improved even more.
	
\pagebreak

# References

\begingroup
\noindent
\vspace{-2em}
\setlength{\parindent}{-0.4in}
\setlength{\leftskip}{0.4in}
\setlength{\parskip}{7pt}

<div id="refs"></div>.

\endgroup

\newpage

\beginsupplement

# Supplement

```{r Table S1 logistic regressions statistical reporting errors, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
hyp_table_err <- c("-1.820", ".157", "-11.573", "< .001", ".162 [.118, .218]", #intercept H1
                 "-.073", ".278", "-.262", ".793", ".930 [.531, 1.585]", #relation hypothesis H1
                 "-4.422", ".503", "-8.791", "< .001", ".012 [.004, .028]", #intercept H2
                 ".708", ".714", ".993", ".321", "2.030 [.475, 8.685]") #relation hypothesis H2
#Putting content table cells into a matrix.
hyp_table_err <- matrix(hyp_table_err, ncol = 5, nrow = 4, byrow = TRUE)

#Giving matrix row and column names.
colnames(hyp_table_err) <- c("b", "SE", "z",  "p", "OR [95% CI]")
rownames(hyp_table_err) <- paste(c("Intercept", "Result hypothesis", "Intercept", "Result hypothesis"))

#Creating Table S1.
hyp_table_err %>%
  #booktabs used, centered alignment, caption
kable(booktabs = T, align = "c", caption = "Logistic regressions for H1 and H2, which concern the difference in prevalence of (gross) inconsistencies in results related to hypotheses versus results not related to hypotheses.") %>% 
  #grouping data for the two hypotheses
  pack_rows("Inconsistencies (H1)", 1, 2) %>%
  pack_rows("Gross inconsistencies (H2)", 3, 4) %>%
  #table will appear in correspondence with its place in this document
  kable_styling(latex_options = c("HOLD_position", "striped"), full_width = F)
```

\hspace{20em}
\newline


```{r Table S2 statistical reporting errors including results level, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
data_table6_S2 <- c("", "", "", "", "", "",                                   #begin APA
                 "", "", "", 168, "22 (13.1%)", "4 (2.4%)",                   #relation to hypothesis
                 "", "", "", 337, "47 (13.9%)", "4 (1.2%)",                   #no relation to hypothesis
                 "", "", "", "", "", "",
                 7, "1 (14.3%)", "1 (14.3%)", 43, "1 (2.3%)", "1 (2.3%)",
                 3, "1 (33.3%)", "0 (0%)", 41, "2 (4.9%)", "0 (0%)",
                 2, "2 (100%)", "0 (0%)", 5, "5 (100%)", "0 (0%)", 
                 36, "12 (33.3%)", "2 (5.6%)", 185, "27 (14.6%)", "3 (1.6%)",
                 28, "13 (46.4%)", "2 (7.1%)", 231, "34 (14.7%)", "4 (1.7%)",
                 "", "", "", "", "", "", 
                 20, "7 (35%)", "1 (5.0%)", 172, "22 (12.8%)", "1 (0.6%)", 
                 21, "7 (33.3%)", "2 (9.5%)", 136, "15 (11.0%)", "3 (2.2%)",
                 35, "15 (42.9%)", "2 (5.7%)", 197, "32 (16.2%)", "4 (2.0%)", 
                 76, "29 (38.2%)", "5 (6.6%)", 505, "69 (13.7%)", "8 (1.6%)", #total, end APA
                 "", "", "", "", "", "",                                      #begin Hyp
                 10, "4 (40.0%)", "1 (10.0%)", 331, "11 (3.3%)", "1 (0.3%)",       #ASR
                 7, "1 (14.3%)", "1 (14.3%)", 68, "1 (1.5%)", "1 (1.5%)",          #AJS
                 2, "1 (50.0%)", "0 (0%)", 5, "2 (40.0%)", "0 (0%)",               #SQ
                 "", "", "", "", "", "",
                 11, "4 (36.4%)", "1 (9.1%)", 312, "11 (3.5%)", "1 (0.3%)",        #2014
                 3, "0 (0%)", "0 (0%)", 33, "0 (0%)", "0 (0%)",                    #2015
                 5, "2 (40.0%)", "1 (20.0%)", 59, "3 (5.1%)", "1 (1.7%)",          #2016
                 19, "6 (31.6%)", "2 (10.5%)", 404, "14 (3.5%)", "2 (0.5%)") #total, end Hyp
#Putting content table cells into a matrix.
table6_S2 <- matrix(data_table6_S2, ncol = 6, nrow = 23, byrow = TRUE)

#Giving matrix row and column names.
rownames(table6_S2) <- c("Relation to hypothesis", "Yes", "No", "Journal", 
                            "ASR", "AJS", "SQ", "JMF", "CHQ", "Year", 
                            "2014", "2015", "2016", "Total", "Journal", 
                            "ASR", "AJS", "SQ", "Year", 
                            "2014", "2015", "2016", "Total")
colnames(table6_S2) <- paste(c("Total", "With inconsistencies", "With gross inconsistencies", "Total", "Inconsistencies", "Gross inconsistencies"))

#Creating Table S2.
table6_S2 %>%
  #booktabs used, centered alignment, caption
  kable(booktabs = T, align = "c", caption = "Descriptive statistics on (gross) inconsistencies for ‘APA' and ‘Hyp'.") %>% 
  #grouping columns based on binwidth
  add_header_above(c(" ", "Article level" = 3, "Results level" = 3)) %>% 
  #adding indentation for the categories of all variables
  add_indent(c(2,3, 5:9, 11:13, 16:18, 20:22)) %>% 
  #grouping data for the two data sets
  pack_rows("‘APA'", 1, 14) %>%
  pack_rows("‘Hyp'", 15, 23) %>%
  #table will appear in correspondence with its place in this document, table is scaled-down to fit page size
  kable_styling(latex_options = c("striped", "HOLD_position", "scale_down"), full_width = F)
```

\hspace{20em}
\newline


```{r Table S3 logistic regressions publication bias, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
h3ab_table <- c("-.205", ".287", "-.713", ".476", ".815 [.456, 1.428]",  #intercept H3a 
                 "-.132", ".504", "-.261", ".794", ".877 [.321, 2.345]", #relation hypothesis H3a
                 ".089", ".211", ".421", ".673", "1.093 [.723, 1.658]", #intercept H3b
                 "-.251", ".392", "-.642", ".521", ".778 [.358, 1.674]") #relation hypothesis Hb
#Putting content table cells into a matrix.
h3ab_table <- matrix(h3ab_table, ncol = 5, nrow = 4, byrow = TRUE)

#Giving matrix row and column names.
colnames(h3ab_table) <- c("b", "SE", "z", "p", "OR [95% CI]")
rownames(h3ab_table) <- paste(c("Intercept", "Result hypothesis", "Intercept", "Result hypothesis"))

#Creating Table S3.
h3ab_table %>%
  #booktabs used, centered alignment, caption
kable(booktabs = T, align = "c", caption = "Logistic regressions for H3a and H3b, which concerns the difference in publication bias between results related to hypotheses versus results not related to hypotheses.") %>% 
  #grouping data for the two hypotheses
  pack_rows("Binwidth .01 (H3a)", 1, 2) %>%
  pack_rows("Binwidth .02 (H3b)", 3, 4) %>%
  #table will appear in correspondence with its place in this document
  kable_styling(latex_options = c("HOLD_position", "striped"), full_width = F)
```
\hspace{20em}
\newline

```{r Table S4 logistic regression marginal significance, echo = FALSE, warning = FALSE, message = FALSE, include=knitr::is_latex_output()}
#Creating content table cells.
h4_table <- c("-.480", ".177", "-2.718", ".007", ".619 [.436, .871]",  #intercept H4 
                 "-.437", ".318", "-1.373", ".170", ".646 [.342, 1.194]") #relation hypothesis H4
#Putting content table cells into a matrix.
h4_table <- matrix(h4_table, ncol = 5, nrow = 2, byrow = TRUE)

#Giving matrix row names.
colnames(h4_table) <- c("b", "SE", "z", "p", "OR [95% CI]")
rownames(h4_table) <- paste(c("Intercept", "Result hypothesis"))


#Creating Table S4.
h4_table %>%
  #centered alignment, caption
kable(booktabs = T, align = "c", caption = "Logistic regression for H4, which concerns the difference in prevalence of marginal significance in results related to hypotheses versus results not related to hypotheses.") %>%   
  #table will appear in correspondence with its place in this document 
  kable_styling(latex_options = c("HOLD_position"), full_width = F)
```



